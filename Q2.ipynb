{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rosebaekfany/Machine-Learning/blob/main/Q2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChC3RF8meAlK"
      },
      "source": [
        "<h1 align=\"center\">Introduction to Machine Learning - 25737-2</h1>\n",
        "<h4 align=\"center\">Dr. R. Amiri</h4>\n",
        "<h4 align=\"center\">Sharif University of Technology, Spring 2024</h4>\n",
        "\n",
        "\n",
        "**<font color='red'>Plagiarism is strongly prohibited!</font>**\n",
        "\n",
        "\n",
        "**Student Name**: Zahra Maleki\n",
        "\n",
        "**Student ID**: 400110009\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IraiR0SbeDi_"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRQjwWC3eDnc"
      },
      "source": [
        "**Task:** Implement your own Logistic Regression model, and test it on the given dataset of Logistic_question.csv!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WD9PvATrMCZ2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "from sklearn.impute import SimpleImputer\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9txoHVytMCZ3"
      },
      "source": [
        "I need to use torch or cupy to makes it runable on gpu.\n",
        "I use torch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8C8cYmGMCZ3"
      },
      "outputs": [],
      "source": [
        "class MyLogisticRegression:\n",
        "    # Your code goes here!\n",
        "    # This class must have an __init__ method, a loss function, a fit function, and a predict function. You also need to make your code runnable on gpu!\n",
        "\n",
        "    def __init__(self, learning_rate=0.05, num_epochs=200, batch_size=32):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_epochs = num_epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.scaler = StandardScaler()\n",
        "        self.model = nn.Linear(7, 1)\n",
        "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
        "        self.optimizer = optim.SGD(self.model.parameters(), lr=self.learning_rate)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X_scaled = self.scaler.fit_transform(X)\n",
        "\n",
        "        X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
        "        y_tensor = torch.tensor(y.values.reshape(-1, 1), dtype=torch.float32)\n",
        "        dataset = torch.utils.data.TensorDataset(X_tensor, y_tensor)\n",
        "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model.to(device)\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            for i, (inputs, targets) in enumerate(dataloader):\n",
        "\n",
        "                inputs = inputs.to(device)\n",
        "                targets = targets.to(device)\n",
        "                outputs = self.model(inputs)\n",
        "                loss = self.loss(outputs, targets)\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "                print(f\"Epoch {epoch + 1}/{self.num_epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        X_scaled = self.scaler.transform(X)\n",
        "\n",
        "        X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        X_tensor = X_tensor.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(X_tensor)\n",
        "\n",
        "        y_pred = torch.sigmoid(outputs).cpu().numpy()\n",
        "\n",
        "        return y_pred.flatten()\n",
        "\n",
        "    def loss(self, y_pred, y_true):\n",
        "\n",
        "        return self.loss_fn(y_pred, y_true)\n",
        "\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-i-oubUlZ6e"
      },
      "source": [
        "**Task:** Test your model on the given dataset. You must split your data into train and test, with a 0.2 split, then normalize your data using X_train data. Finally, report 4 different evaluation metrics of the model on the test set. (You might want to first make the Target column binary!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ac6FeDx-MCZ3",
        "outputId": "58395581-8d88-4882-f365-0eb7f1d0eacd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Feature 1  Feature 2  Feature 3  Feature 4  Feature 5  Feature 6  \\\n",
            "0         47       11.8          4        4.5        4.5       9.65   \n",
            "1         34       10.7          4        4.0        4.5       8.87   \n",
            "2         26       10.4          3        3.0        3.5       8.00   \n",
            "3         32       11.0          3        3.5        2.5       8.67   \n",
            "4         24       10.3          2        2.0        3.0       8.21   \n",
            "\n",
            "   Feature 7  Target  \n",
            "0          1    0.92  \n",
            "1          1    0.76  \n",
            "2          1    0.72  \n",
            "3          1    0.80  \n",
            "4          0    0.65  \n"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv('Logistic_question.csv')\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KXzIy_2u-pG",
        "outputId": "9625f7e2-abb1-4591-c0fa-843525e0ffd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/200, Loss: 0.2779524624347687\n",
            "Epoch 20/200, Loss: 0.29011520743370056\n",
            "Epoch 30/200, Loss: 0.1773461401462555\n",
            "Epoch 40/200, Loss: 0.19269169867038727\n",
            "Epoch 50/200, Loss: 0.3599705994129181\n",
            "Epoch 60/200, Loss: 0.1182558536529541\n",
            "Epoch 70/200, Loss: 0.2611558437347412\n",
            "Epoch 80/200, Loss: 0.14203639328479767\n",
            "Epoch 90/200, Loss: 0.052083007991313934\n",
            "Epoch 100/200, Loss: 0.10353375226259232\n",
            "Epoch 110/200, Loss: 0.14447498321533203\n",
            "Epoch 120/200, Loss: 0.08275137096643448\n",
            "Epoch 130/200, Loss: 0.20399662852287292\n",
            "Epoch 140/200, Loss: 0.10700931400060654\n",
            "Epoch 150/200, Loss: 0.08751626312732697\n",
            "Epoch 160/200, Loss: 0.10247841477394104\n",
            "Epoch 170/200, Loss: 0.1054399162530899\n",
            "Epoch 180/200, Loss: 0.10184996575117111\n",
            "Epoch 190/200, Loss: 0.20683801174163818\n",
            "Epoch 200/200, Loss: 0.06633664667606354\n",
            "Accuracy: 0.925\n",
            "Precision: 0.9210526315789473\n",
            "Recall: 1.0\n",
            "F1 Score: 0.958904109589041\n"
          ]
        }
      ],
      "source": [
        "data['Target'] = data['Target'].apply(lambda x: 1 if x > 0.5 else 0)\n",
        "X = data.drop('Target', axis=1)\n",
        "y = data['Target']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "model = MyLogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred_binary)\n",
        "precision = precision_score(y_test, y_pred_binary)\n",
        "recall = recall_score(y_test, y_pred_binary)\n",
        "f1 = f1_score(y_test, y_pred_binary)\n",
        "\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ji0RXNGKv1pa"
      },
      "source": [
        "**Question:** What are each of your used evaluation metrics? And for each one, mention situations in which they convey more data on the model performance in specific tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldveD35twRRZ"
      },
      "source": [
        "**Your answer:**\n",
        "\n",
        "Accuracy: the proportion of correctly predicted samples in a test set. Accuracy is suitable for balanced datasets and tasks where all classes need to be classified correctly.\n",
        "\n",
        "Precision: Measures the proportion of correctly predicted positive samples out of all predicted positives. Useful when minimizing false positives is important, such as in medical diagnosis.\n",
        "\n",
        "Recall: Calculates the proportion of correctly predicted positive samples out of all actual positives. Useful when minimizing false negatives is crucial, such as in fraud detection or rare disease identification.\n",
        "\n",
        "F1 Score: The harmonic mean of precision and recall, balancing both metrics. Suitable when considering false positives and false negatives is important, such as in information retrieval or text classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZCeRHZSw-mh"
      },
      "source": [
        "**Task:** Now test the built-in function of Python for Logistic Regression, and report all the same metrics used before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vb5lRSQXDLR3",
        "outputId": "66704498-fe19-473b-a926-ca7a838b37b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9500\n",
            "Precision: 0.9459\n",
            "Recall: 1.0000\n",
            "F1-score: 0.9722\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv('Logistic_question.csv')\n",
        "data['Target'] = data['Target'].apply(lambda x: 1 if x > 0.5 else 0)\n",
        "X = data.drop('Target', axis=1).values\n",
        "y = data['Target'].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCvIymmMy_ji"
      },
      "source": [
        "**Question:** Compare your function with the built-in function. On the matters of performance and parameters. Briefly explain what the parameters of the built-in function are and how they affect the model's performance?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EY0ohM16z3De"
      },
      "source": [
        "**Your answer:**\n",
        "\n",
        "Penalty: Determines the type of regularization to be applied (L1 or L2).\n",
        "\n",
        "C: Controls the trade-off between fitting the training data and preventing overfitting.\n",
        "\n",
        "Solver: Determines the algorithm used to solve the optimization problem.\n",
        "\n",
        "Max_iter: Sets the maximum number of iterations for convergence.\n",
        "\n",
        "Class_weight: Allows for assigning different weights to different classes in the target variable.\n",
        "\n",
        "These parameters impact the model's complexity, convergence speed, handling of large datasets, and ability to handle class imbalances.\n",
        "\n",
        "A custom logistic regression function provides flexibility and insight into the model's inner workings, the built-in logistic regression function from Scikit-learn offers several advantages. It provides optimized performance, advanced features, and efficient parameter tuning capabilities. This makes it a preferred choice for practical applications, especially with large-scale datasets. The built-in function is designed to handle various scenarios effectively and is well-optimized for performance, allowing for more efficient training and prediction processes. Additionally, Scikit-learn provides convenient tools for cross-validation and hyperparameter tuning, enabling users to find the optimal parameter values for their specific problem easily."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClMqoYlr2kr7"
      },
      "source": [
        "# Multinomial Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukvlqDe52xP5"
      },
      "source": [
        "**Task:** Implement your own Multinomial Logistic Regression model. Your model must be able to handle any number of labels!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Ir-_hFt286t"
      },
      "outputs": [],
      "source": [
        "class MyMultinomialLogisticRegression:\n",
        "\n",
        "    def __init__(self, learning_rate=0.05, num_epochs=200, batch_size=32):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_epochs = num_epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.scaler = StandardScaler()\n",
        "        self.model = None\n",
        "        self.loss_fn = nn.CrossEntropyLoss()\n",
        "        self.optimizer = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        num_features = X.shape[1]\n",
        "        num_classes = len(set(y))\n",
        "\n",
        "        self.model = nn.Linear(num_features, num_classes)\n",
        "        self.optimizer = optim.SGD(self.model.parameters(), lr=self.learning_rate)\n",
        "\n",
        "        X_scaled = self.scaler.fit_transform(X)\n",
        "        X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
        "        y_tensor = torch.tensor(y, dtype=torch.long)\n",
        "        dataset = torch.utils.data.TensorDataset(X_tensor, y_tensor)\n",
        "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model.to(device)\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            for inputs, targets in dataloader:\n",
        "                inputs = inputs.to(device)\n",
        "                targets = targets.to(device)\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                outputs = self.model(inputs)\n",
        "                loss = self.loss_fn(outputs, targets)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "                print(f\"Epoch {epoch + 1}/{self.num_epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        X_scaled = self.scaler.transform(X)\n",
        "        X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        X_tensor = X_tensor.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(X_tensor)\n",
        "\n",
        "        _, predicted_labels = torch.max(outputs, 1)\n",
        "        predicted_labels = predicted_labels.cpu().numpy()\n",
        "\n",
        "        return predicted_labels\n",
        "\n",
        "    def loss(self, y_pred, y_true):\n",
        "        return self.loss_fn(y_pred, y_true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPQ3Rtay3Y2_"
      },
      "source": [
        "**Task:** Test your model on the given dataset. Do the same as the previous part, but here you might want to first make the Target column quantized into $i$ levels. Change $i$ from 2 to 10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aP4QJPq29B3",
        "outputId": "d8dfb484-96ce-4510-a4a1-837eca09dd6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quantization Level: 2\n",
            "Epoch 10/200, Loss: 0.27591943740844727\n",
            "Epoch 20/200, Loss: 0.16800615191459656\n",
            "Epoch 30/200, Loss: 0.1571761518716812\n",
            "Epoch 40/200, Loss: 0.09795619547367096\n",
            "Epoch 50/200, Loss: 0.15889331698417664\n",
            "Epoch 60/200, Loss: 0.19519264996051788\n",
            "Epoch 70/200, Loss: 0.20060524344444275\n",
            "Epoch 80/200, Loss: 0.10897186398506165\n",
            "Epoch 90/200, Loss: 0.2550385892391205\n",
            "Epoch 100/200, Loss: 0.13332341611385345\n",
            "Epoch 110/200, Loss: 0.22735659778118134\n",
            "Epoch 120/200, Loss: 0.12178438156843185\n",
            "Epoch 130/200, Loss: 0.185542032122612\n",
            "Epoch 140/200, Loss: 0.07059450447559357\n",
            "Epoch 150/200, Loss: 0.15709732472896576\n",
            "Epoch 160/200, Loss: 0.2889235019683838\n",
            "Epoch 170/200, Loss: 0.059724606573581696\n",
            "Epoch 180/200, Loss: 0.10155331343412399\n",
            "Epoch 190/200, Loss: 0.11380846053361893\n",
            "Epoch 200/200, Loss: 0.10137495398521423\n",
            "Accuracy: 0.95\n",
            "Quantization Level: 3\n",
            "Epoch 10/200, Loss: 0.26157528162002563\n",
            "Epoch 20/200, Loss: 0.21668313443660736\n",
            "Epoch 30/200, Loss: 0.23102644085884094\n",
            "Epoch 40/200, Loss: 0.1673968881368637\n",
            "Epoch 50/200, Loss: 0.15642882883548737\n",
            "Epoch 60/200, Loss: 0.05613602697849274\n",
            "Epoch 70/200, Loss: 0.0676814466714859\n",
            "Epoch 80/200, Loss: 0.15946602821350098\n",
            "Epoch 90/200, Loss: 0.14971692860126495\n",
            "Epoch 100/200, Loss: 0.1566319316625595\n",
            "Epoch 110/200, Loss: 0.14727424085140228\n",
            "Epoch 120/200, Loss: 0.10532107949256897\n",
            "Epoch 130/200, Loss: 0.17961573600769043\n",
            "Epoch 140/200, Loss: 0.2110283076763153\n",
            "Epoch 150/200, Loss: 0.22002293169498444\n",
            "Epoch 160/200, Loss: 0.09759805351495743\n",
            "Epoch 170/200, Loss: 0.20673666894435883\n",
            "Epoch 180/200, Loss: 0.1646614819765091\n",
            "Epoch 190/200, Loss: 0.08710852265357971\n",
            "Epoch 200/200, Loss: 0.1022484079003334\n",
            "Accuracy: 0.95\n",
            "Quantization Level: 4\n",
            "Epoch 10/200, Loss: 0.3178165853023529\n",
            "Epoch 20/200, Loss: 0.17753994464874268\n",
            "Epoch 30/200, Loss: 0.2099100798368454\n",
            "Epoch 40/200, Loss: 0.22769542038440704\n",
            "Epoch 50/200, Loss: 0.19471177458763123\n",
            "Epoch 60/200, Loss: 0.23966684937477112\n",
            "Epoch 70/200, Loss: 0.1609138697385788\n",
            "Epoch 80/200, Loss: 0.08247855305671692\n",
            "Epoch 90/200, Loss: 0.13078247010707855\n",
            "Epoch 100/200, Loss: 0.2551129162311554\n",
            "Epoch 110/200, Loss: 0.12303967773914337\n",
            "Epoch 120/200, Loss: 0.08790791034698486\n",
            "Epoch 130/200, Loss: 0.23715835809707642\n",
            "Epoch 140/200, Loss: 0.08834351599216461\n",
            "Epoch 150/200, Loss: 0.11075036972761154\n",
            "Epoch 160/200, Loss: 0.2605820596218109\n",
            "Epoch 170/200, Loss: 0.05298985168337822\n",
            "Epoch 180/200, Loss: 0.15212281048297882\n",
            "Epoch 190/200, Loss: 0.09565388411283493\n",
            "Epoch 200/200, Loss: 0.12588809430599213\n",
            "Accuracy: 0.95\n",
            "Quantization Level: 5\n",
            "Epoch 10/200, Loss: 0.30308809876441956\n",
            "Epoch 20/200, Loss: 0.22439563274383545\n",
            "Epoch 30/200, Loss: 0.12882958352565765\n",
            "Epoch 40/200, Loss: 0.268811970949173\n",
            "Epoch 50/200, Loss: 0.23637840151786804\n",
            "Epoch 60/200, Loss: 0.2551959753036499\n",
            "Epoch 70/200, Loss: 0.18939372897148132\n",
            "Epoch 80/200, Loss: 0.23227183520793915\n",
            "Epoch 90/200, Loss: 0.1767243593931198\n",
            "Epoch 100/200, Loss: 0.07834930717945099\n",
            "Epoch 110/200, Loss: 0.06422169506549835\n",
            "Epoch 120/200, Loss: 0.14754068851470947\n",
            "Epoch 130/200, Loss: 0.12485863268375397\n",
            "Epoch 140/200, Loss: 0.16576169431209564\n",
            "Epoch 150/200, Loss: 0.12476852536201477\n",
            "Epoch 160/200, Loss: 0.2704467475414276\n",
            "Epoch 170/200, Loss: 0.15737822651863098\n",
            "Epoch 180/200, Loss: 0.055312227457761765\n",
            "Epoch 190/200, Loss: 0.15452030301094055\n",
            "Epoch 200/200, Loss: 0.10290388762950897\n",
            "Accuracy: 0.95\n",
            "Quantization Level: 6\n",
            "Epoch 10/200, Loss: 0.22715629637241364\n",
            "Epoch 20/200, Loss: 0.15014374256134033\n",
            "Epoch 30/200, Loss: 0.20079557597637177\n",
            "Epoch 40/200, Loss: 0.1148848608136177\n",
            "Epoch 50/200, Loss: 0.2416892945766449\n",
            "Epoch 60/200, Loss: 0.3312472105026245\n",
            "Epoch 70/200, Loss: 0.10717515647411346\n",
            "Epoch 80/200, Loss: 0.06474658101797104\n",
            "Epoch 90/200, Loss: 0.15333054959774017\n",
            "Epoch 100/200, Loss: 0.13412494957447052\n",
            "Epoch 110/200, Loss: 0.044990722090005875\n",
            "Epoch 120/200, Loss: 0.22529777884483337\n",
            "Epoch 130/200, Loss: 0.09782658517360687\n",
            "Epoch 140/200, Loss: 0.09441548585891724\n",
            "Epoch 150/200, Loss: 0.23705017566680908\n",
            "Epoch 160/200, Loss: 0.10490947961807251\n",
            "Epoch 170/200, Loss: 0.1741066724061966\n",
            "Epoch 180/200, Loss: 0.12034094333648682\n",
            "Epoch 190/200, Loss: 0.24516133964061737\n",
            "Epoch 200/200, Loss: 0.166881263256073\n",
            "Accuracy: 0.9375\n",
            "Quantization Level: 7\n",
            "Epoch 10/200, Loss: 0.24043285846710205\n",
            "Epoch 20/200, Loss: 0.1582842618227005\n",
            "Epoch 30/200, Loss: 0.19892239570617676\n",
            "Epoch 40/200, Loss: 0.22478969395160675\n",
            "Epoch 50/200, Loss: 0.08346637338399887\n",
            "Epoch 60/200, Loss: 0.19869564473628998\n",
            "Epoch 70/200, Loss: 0.1659654676914215\n",
            "Epoch 80/200, Loss: 0.2253040075302124\n",
            "Epoch 90/200, Loss: 0.10027911514043808\n",
            "Epoch 100/200, Loss: 0.12619386613368988\n",
            "Epoch 110/200, Loss: 0.11942492425441742\n",
            "Epoch 120/200, Loss: 0.09508238732814789\n",
            "Epoch 130/200, Loss: 0.19318628311157227\n",
            "Epoch 140/200, Loss: 0.17675504088401794\n",
            "Epoch 150/200, Loss: 0.043570127338171005\n",
            "Epoch 160/200, Loss: 0.14163871109485626\n",
            "Epoch 170/200, Loss: 0.18331632018089294\n",
            "Epoch 180/200, Loss: 0.2190117985010147\n",
            "Epoch 190/200, Loss: 0.1728416383266449\n",
            "Epoch 200/200, Loss: 0.20585627853870392\n",
            "Accuracy: 0.95\n",
            "Quantization Level: 8\n",
            "Epoch 10/200, Loss: 0.25135722756385803\n",
            "Epoch 20/200, Loss: 0.26154905557632446\n",
            "Epoch 30/200, Loss: 0.18321675062179565\n",
            "Epoch 40/200, Loss: 0.22674895823001862\n",
            "Epoch 50/200, Loss: 0.14226242899894714\n",
            "Epoch 60/200, Loss: 0.08452748507261276\n",
            "Epoch 70/200, Loss: 0.07790374755859375\n",
            "Epoch 80/200, Loss: 0.12958557903766632\n",
            "Epoch 90/200, Loss: 0.07424259185791016\n",
            "Epoch 100/200, Loss: 0.17199969291687012\n",
            "Epoch 110/200, Loss: 0.1876818835735321\n",
            "Epoch 120/200, Loss: 0.04763875901699066\n",
            "Epoch 130/200, Loss: 0.05558950453996658\n",
            "Epoch 140/200, Loss: 0.222556009888649\n",
            "Epoch 150/200, Loss: 0.22459952533245087\n",
            "Epoch 160/200, Loss: 0.14927995204925537\n",
            "Epoch 170/200, Loss: 0.1780359297990799\n",
            "Epoch 180/200, Loss: 0.08144278079271317\n",
            "Epoch 190/200, Loss: 0.15754427015781403\n",
            "Epoch 200/200, Loss: 0.10260452330112457\n",
            "Accuracy: 0.95\n",
            "Quantization Level: 9\n",
            "Epoch 10/200, Loss: 0.19184622168540955\n",
            "Epoch 20/200, Loss: 0.15916146337985992\n",
            "Epoch 30/200, Loss: 0.19538140296936035\n",
            "Epoch 40/200, Loss: 0.20030929148197174\n",
            "Epoch 50/200, Loss: 0.14938175678253174\n",
            "Epoch 60/200, Loss: 0.15437230467796326\n",
            "Epoch 70/200, Loss: 0.15175391733646393\n",
            "Epoch 80/200, Loss: 0.10010078549385071\n",
            "Epoch 90/200, Loss: 0.2741346061229706\n",
            "Epoch 100/200, Loss: 0.1964818239212036\n",
            "Epoch 110/200, Loss: 0.11311730742454529\n",
            "Epoch 120/200, Loss: 0.1189771443605423\n",
            "Epoch 130/200, Loss: 0.07763392478227615\n",
            "Epoch 140/200, Loss: 0.20439444482326508\n",
            "Epoch 150/200, Loss: 0.3104165494441986\n",
            "Epoch 160/200, Loss: 0.16930873692035675\n",
            "Epoch 170/200, Loss: 0.2083756923675537\n",
            "Epoch 180/200, Loss: 0.20515713095664978\n",
            "Epoch 190/200, Loss: 0.13070696592330933\n",
            "Epoch 200/200, Loss: 0.151373490691185\n",
            "Accuracy: 0.9375\n",
            "Quantization Level: 10\n",
            "Epoch 10/200, Loss: 0.18164663016796112\n",
            "Epoch 20/200, Loss: 0.2384447455406189\n",
            "Epoch 30/200, Loss: 0.20271429419517517\n",
            "Epoch 40/200, Loss: 0.2180136889219284\n",
            "Epoch 50/200, Loss: 0.09345190972089767\n",
            "Epoch 60/200, Loss: 0.13933241367340088\n",
            "Epoch 70/200, Loss: 0.1644519418478012\n",
            "Epoch 80/200, Loss: 0.19634529948234558\n",
            "Epoch 90/200, Loss: 0.08581821620464325\n",
            "Epoch 100/200, Loss: 0.1203499436378479\n",
            "Epoch 110/200, Loss: 0.17344772815704346\n",
            "Epoch 120/200, Loss: 0.08559217303991318\n",
            "Epoch 130/200, Loss: 0.1750243902206421\n",
            "Epoch 140/200, Loss: 0.05424369126558304\n",
            "Epoch 150/200, Loss: 0.2586668133735657\n",
            "Epoch 160/200, Loss: 0.19044868648052216\n",
            "Epoch 170/200, Loss: 0.07332909107208252\n",
            "Epoch 180/200, Loss: 0.13047081232070923\n",
            "Epoch 190/200, Loss: 0.20237873494625092\n",
            "Epoch 200/200, Loss: 0.16342933475971222\n",
            "Accuracy: 0.95\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv('Logistic_question.csv')\n",
        "data['Target'] = data['Target'].apply(lambda x: 1 if x > 0.5 else 0)\n",
        "X = data.drop('Target', axis=1).values\n",
        "y = data['Target'].values\n",
        "\n",
        "for num_levels in range(2, 11):\n",
        "    print(f\"Quantization Level: {num_levels}\")\n",
        "\n",
        "    discretizer = KBinsDiscretizer(n_bins=num_levels, encode=\"ordinal\")\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    model = MyMultinomialLogisticRegression()\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Of2sHl5Z4dXi"
      },
      "source": [
        "**Question:** Report for which $i$ your model performs best. Describe and analyze the results! You could use visualizations or any other method!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRLERDAr4wnS"
      },
      "source": [
        "**Your answer:** Looking at the loss at 200 epoch for i=2 we have a better performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wT43jGKV6CBZ"
      },
      "source": [
        "# Going a little further!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vo9uGo0R6GZo"
      },
      "source": [
        "First we download Adult income dataset from Kaggle! In order to do this create an account on this website, and create an API. A file named kaggle.json will be downloaded to your device. Then use the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "o-vrjYBF7u1E",
        "outputId": "b274bc6e-4c35-4ad8-f17b-9e69f7d92923"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[37], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m files\n\u001b[0;32m      2\u001b[0m files\u001b[38;5;241m.\u001b[39mupload()  \u001b[38;5;66;03m# Use this to select the kaggle.json file from your computer\u001b[39;00m\n\u001b[0;32m      3\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmkdir -p ~/.kaggle\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.upload()  # Use this to select the kaggle.json file from your computer\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5i6u6_1v8ftX"
      },
      "source": [
        "Then use this code to automatically download the dataset into Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjyVaVKF29Hx",
        "outputId": "15d0b1a2-c806-4102-abbc-12545237e218"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'kaggle' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "unzip:  cannot find either /content/adult-income-dataset.zip or /content/adult-income-dataset.zip.zip.\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d wenruliu/adult-income-dataset\n",
        "!unzip /content/adult-income-dataset.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXQnbZwt8rJK"
      },
      "source": [
        "**Task:** Determine the number of null entries!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtuEx6QW29c1",
        "outputId": "43397bec-0622-4dc4-de2b-c65be00e4503"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   age  workclass  fnlwgt     education  educational-num      marital-status  \\\n",
            "0   25    Private  226802          11th                7       Never-married   \n",
            "1   38    Private   89814       HS-grad                9  Married-civ-spouse   \n",
            "2   28  Local-gov  336951    Assoc-acdm               12  Married-civ-spouse   \n",
            "3   44    Private  160323  Some-college               10  Married-civ-spouse   \n",
            "4   18          ?  103497  Some-college               10       Never-married   \n",
            "\n",
            "          occupation relationship   race  gender  capital-gain  capital-loss  \\\n",
            "0  Machine-op-inspct    Own-child  Black    Male             0             0   \n",
            "1    Farming-fishing      Husband  White    Male             0             0   \n",
            "2    Protective-serv      Husband  White    Male             0             0   \n",
            "3  Machine-op-inspct      Husband  Black    Male          7688             0   \n",
            "4                  ?    Own-child  White  Female             0             0   \n",
            "\n",
            "   hours-per-week native-country income  \n",
            "0              40  United-States  <=50K  \n",
            "1              50  United-States  <=50K  \n",
            "2              40  United-States   >50K  \n",
            "3              40  United-States   >50K  \n",
            "4              30  United-States  <=50K  \n",
            "age                0\n",
            "workclass          0\n",
            "fnlwgt             0\n",
            "education          0\n",
            "educational-num    0\n",
            "marital-status     0\n",
            "occupation         0\n",
            "relationship       0\n",
            "race               0\n",
            "gender             0\n",
            "capital-gain       0\n",
            "capital-loss       0\n",
            "hours-per-week     0\n",
            "native-country     0\n",
            "income             0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv('adult.csv')\n",
        "print(data.head())\n",
        "null_counts = data.isnull().sum()\n",
        "print(null_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpEcBdTUAYVN"
      },
      "source": [
        "**Question:** In many widely used datasets there are a lot of null entries. Propose 5 methods by which, one could deal with this problem. Briefly explain how do you decide which one to use in this problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1u1pBHuAsSg"
      },
      "source": [
        "**Your answer:**\n",
        "\n",
        "Dropping Rows or Columns: Remove rows or columns with null values. Suitable when null values are limited and won't significantly impact analysis.\n",
        "\n",
        "Imputation: Fill null values with estimated or calculated values (e.g., mean, median, mode). Simple techniques or advanced methods like regression or k-nearest neighbors imputation.\n",
        "\n",
        "Forward Fill or Backward Fill: Fill null values with nearest preceding or succeeding non-null value. Useful for time series or ordered data.\n",
        "\n",
        "Creating Indicator Variables: Add binary column to indicate missing values. Helps capture patterns or relationships associated with missing data.\n",
        "\n",
        "Domain-specific Substitution: Use domain knowledge to substitute null values with reasonable estimates based on dataset and problem understanding.\n",
        "\n",
        "The choice of method depends on factors like missing data amount, pattern, dataset nature, analysis goals, and downstream impact."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHhH-hkpAxFf"
      },
      "source": [
        "**Task:** Handle null entries using your best method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fVwWcjK29fk"
      },
      "outputs": [],
      "source": [
        "columns_with_null = data.columns[data.isnull().any()]\n",
        "if len(columns_with_null) > 0:\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "    data[columns_with_null] = imputer.fit_transform(data[columns_with_null])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43k5cTorCJaV"
      },
      "source": [
        "**Task:** Convert categorical features to numerical values. Split the dataset with 80-20 portion. Normalize all the data using X_train. Use the built-in Logistic Regression function and GridSearchCV to train your model, and report the parameters, train and test accuracy of the best model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Agj18Lcd-vyZ",
        "outputId": "69e132a9-0249-4a21-c8f3-45247c1e17dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Parameters: {'C': 1, 'penalty': 'l2', 'solver': 'saga'}\n",
            "Train Accuracy: 0.95\n",
            "Test Accuracy: 0.95\n"
          ]
        }
      ],
      "source": [
        "X1 = pd.DataFrame(X)\n",
        "categorical_features = X1.select_dtypes(include=['object']).columns\n",
        "\n",
        "label_encoders = {}\n",
        "for feature in categorical_features:\n",
        "    label_encoders[feature] = LabelEncoder()\n",
        "    X[feature] = label_encoders[feature].fit_transform(X[feature])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "param_grid = {'C': [0.1, 1, 10], 'penalty': ['l1', 'l2'], 'solver': ['liblinear', 'saga']}\n",
        "\n",
        "\n",
        "model = LogisticRegression()\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "y_train_pred = best_model.predict(X_train)\n",
        "y_test_pred = best_model.predict(X_test)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Train Accuracy:\", train_accuracy)\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Lzr2lqXDQ1T"
      },
      "source": [
        "**Task:** To try a different route, split X_train into $i$ parts, and train $i$ separate models on these parts. Now propose and implement 3 different *ensemble methods* to derive the global models' prediction for X_test using the results(not necessarily predictions!) of the $i$ models. Firstly, set $i=10$ to find the method with the best test accuracy(the answer is not general!). You must Use your own Logistic Regression model.(You might want to modify it a little bit for this part!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXssIVAAMCZ7"
      },
      "outputs": [],
      "source": [
        "class MyLogisticRegression:\n",
        "    # Your code goes here!\n",
        "    # This class must have an __init__ method, a loss function, a fit function, and a predict function. You also need to make your code runnable on gpu!\n",
        "\n",
        "    def __init__(self, learning_rate=0.05, num_epochs=200, batch_size=32):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_epochs = num_epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.scaler = StandardScaler()\n",
        "        self.model = nn.Linear(7, 1)\n",
        "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
        "        self.optimizer = optim.SGD(self.model.parameters(), lr=self.learning_rate)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X_scaled = self.scaler.fit_transform(X)\n",
        "\n",
        "        X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
        "        y_tensor = torch.tensor(y, dtype=torch.float32)  # Reshape y to (batch_size, 1)\n",
        "        dataset = torch.utils.data.TensorDataset(X_tensor, y_tensor)\n",
        "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model.to(device)\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            for i, (inputs, targets) in enumerate(dataloader):\n",
        "                inputs = inputs.to(device)\n",
        "                targets = targets.view(-1, 1).to(device)  # Reshape targets to (batch_size, 1)\n",
        "                outputs = self.model(inputs)\n",
        "                loss = self.loss(outputs, targets)\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "                print(f\"Epoch {epoch + 1}/{self.num_epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        X_scaled = self.scaler.transform(X)\n",
        "\n",
        "        X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        X_tensor = X_tensor.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(X_tensor)\n",
        "\n",
        "        y_pred = torch.sigmoid(outputs).cpu().numpy()\n",
        "\n",
        "        return y_pred.flatten()\n",
        "\n",
        "    def loss(self, y_pred, y_true):\n",
        "\n",
        "        return self.loss_fn(y_pred, y_true)\n",
        "\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9D1jlstF9nF",
        "outputId": "b0ae78ce-b874-4ff8-bc37-1c1c90bcb9a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/200, Loss: 0.7182658314704895\n",
            "Epoch 20/200, Loss: 0.6108105778694153\n",
            "Epoch 30/200, Loss: 0.5275748372077942\n",
            "Epoch 40/200, Loss: 0.4622161090373993\n",
            "Epoch 50/200, Loss: 0.41024553775787354\n",
            "Epoch 60/200, Loss: 0.3684062063694\n",
            "Epoch 70/200, Loss: 0.33430564403533936\n",
            "Epoch 80/200, Loss: 0.306175172328949\n",
            "Epoch 90/200, Loss: 0.2826988399028778\n",
            "Epoch 100/200, Loss: 0.2628902196884155\n",
            "Epoch 110/200, Loss: 0.24600344896316528\n",
            "Epoch 120/200, Loss: 0.23146912455558777\n",
            "Epoch 130/200, Loss: 0.2188481092453003\n",
            "Epoch 140/200, Loss: 0.2077983021736145\n",
            "Epoch 150/200, Loss: 0.19805067777633667\n",
            "Epoch 160/200, Loss: 0.18939143419265747\n",
            "Epoch 170/200, Loss: 0.1816493421792984\n",
            "Epoch 180/200, Loss: 0.17468583583831787\n",
            "Epoch 190/200, Loss: 0.1683879792690277\n",
            "Epoch 200/200, Loss: 0.162663072347641\n",
            "Epoch 10/200, Loss: 0.6443040370941162\n",
            "Epoch 20/200, Loss: 0.5584699511528015\n",
            "Epoch 30/200, Loss: 0.49956226348876953\n",
            "Epoch 40/200, Loss: 0.45479583740234375\n",
            "Epoch 50/200, Loss: 0.4190438687801361\n",
            "Epoch 60/200, Loss: 0.3897187113761902\n",
            "Epoch 70/200, Loss: 0.36525505781173706\n",
            "Epoch 80/200, Loss: 0.3445873260498047\n",
            "Epoch 90/200, Loss: 0.32693934440612793\n",
            "Epoch 100/200, Loss: 0.31172460317611694\n",
            "Epoch 110/200, Loss: 0.2984905242919922\n",
            "Epoch 120/200, Loss: 0.2868833839893341\n",
            "Epoch 130/200, Loss: 0.27662402391433716\n",
            "Epoch 140/200, Loss: 0.26749008893966675\n",
            "Epoch 150/200, Loss: 0.2593034505844116\n",
            "Epoch 160/200, Loss: 0.2519199252128601\n",
            "Epoch 170/200, Loss: 0.2452222853899002\n",
            "Epoch 180/200, Loss: 0.23911426961421967\n",
            "Epoch 190/200, Loss: 0.23351648449897766\n",
            "Epoch 200/200, Loss: 0.2283627688884735\n",
            "Epoch 10/200, Loss: 0.6736891269683838\n",
            "Epoch 20/200, Loss: 0.583130955696106\n",
            "Epoch 30/200, Loss: 0.5290317535400391\n",
            "Epoch 40/200, Loss: 0.48976653814315796\n",
            "Epoch 50/200, Loss: 0.4588303864002228\n",
            "Epoch 60/200, Loss: 0.4335768520832062\n",
            "Epoch 70/200, Loss: 0.4125707149505615\n",
            "Epoch 80/200, Loss: 0.39487025141716003\n",
            "Epoch 90/200, Loss: 0.37979382276535034\n",
            "Epoch 100/200, Loss: 0.36682629585266113\n",
            "Epoch 110/200, Loss: 0.3555697798728943\n",
            "Epoch 120/200, Loss: 0.3457135558128357\n",
            "Epoch 130/200, Loss: 0.3370128571987152\n",
            "Epoch 140/200, Loss: 0.3292735517024994\n",
            "Epoch 150/200, Loss: 0.3223402202129364\n",
            "Epoch 160/200, Loss: 0.316087931394577\n",
            "Epoch 170/200, Loss: 0.31041523814201355\n",
            "Epoch 180/200, Loss: 0.3052392899990082\n",
            "Epoch 190/200, Loss: 0.3004917502403259\n",
            "Epoch 200/200, Loss: 0.2961162030696869\n",
            "Epoch 10/200, Loss: 0.5842061638832092\n",
            "Epoch 20/200, Loss: 0.4971383810043335\n",
            "Epoch 30/200, Loss: 0.42859405279159546\n",
            "Epoch 40/200, Loss: 0.3739999532699585\n",
            "Epoch 50/200, Loss: 0.3299746811389923\n",
            "Epoch 60/200, Loss: 0.29402586817741394\n",
            "Epoch 70/200, Loss: 0.26431387662887573\n",
            "Epoch 80/200, Loss: 0.23947320878505707\n",
            "Epoch 90/200, Loss: 0.21848204731941223\n",
            "Epoch 100/200, Loss: 0.20056810975074768\n",
            "Epoch 110/200, Loss: 0.18514126539230347\n",
            "Epoch 120/200, Loss: 0.1717456877231598\n",
            "Epoch 130/200, Loss: 0.16002556681632996\n",
            "Epoch 140/200, Loss: 0.1497000902891159\n",
            "Epoch 150/200, Loss: 0.14054544270038605\n",
            "Epoch 160/200, Loss: 0.13238151371479034\n",
            "Epoch 170/200, Loss: 0.12506219744682312\n",
            "Epoch 180/200, Loss: 0.11846773326396942\n",
            "Epoch 190/200, Loss: 0.11249946057796478\n",
            "Epoch 200/200, Loss: 0.10707519203424454\n",
            "Epoch 10/200, Loss: 0.5826901793479919\n",
            "Epoch 20/200, Loss: 0.5092467069625854\n",
            "Epoch 30/200, Loss: 0.4506450295448303\n",
            "Epoch 40/200, Loss: 0.4033634662628174\n",
            "Epoch 50/200, Loss: 0.36475932598114014\n",
            "Epoch 60/200, Loss: 0.33285626769065857\n",
            "Epoch 70/200, Loss: 0.30617591738700867\n",
            "Epoch 80/200, Loss: 0.28360798954963684\n",
            "Epoch 90/200, Loss: 0.2643129229545593\n",
            "Epoch 100/200, Loss: 0.2476503551006317\n",
            "Epoch 110/200, Loss: 0.23312745988368988\n",
            "Epoch 120/200, Loss: 0.22036103904247284\n",
            "Epoch 130/200, Loss: 0.20905041694641113\n",
            "Epoch 140/200, Loss: 0.19895723462104797\n",
            "Epoch 150/200, Loss: 0.18989092111587524\n",
            "Epoch 160/200, Loss: 0.18169768154621124\n",
            "Epoch 170/200, Loss: 0.1742524802684784\n",
            "Epoch 180/200, Loss: 0.16745269298553467\n",
            "Epoch 190/200, Loss: 0.16121351718902588\n",
            "Epoch 200/200, Loss: 0.15546438097953796\n",
            "Epoch 10/200, Loss: 0.48077088594436646\n",
            "Epoch 20/200, Loss: 0.4310612678527832\n",
            "Epoch 30/200, Loss: 0.3935728073120117\n",
            "Epoch 40/200, Loss: 0.3634852170944214\n",
            "Epoch 50/200, Loss: 0.3386043608188629\n",
            "Epoch 60/200, Loss: 0.3176620602607727\n",
            "Epoch 70/200, Loss: 0.2998051643371582\n",
            "Epoch 80/200, Loss: 0.28441300988197327\n",
            "Epoch 90/200, Loss: 0.27101650834083557\n",
            "Epoch 100/200, Loss: 0.25925344228744507\n",
            "Epoch 110/200, Loss: 0.2488405704498291\n",
            "Epoch 120/200, Loss: 0.23955397307872772\n",
            "Epoch 130/200, Loss: 0.23121491074562073\n",
            "Epoch 140/200, Loss: 0.22367969155311584\n",
            "Epoch 150/200, Loss: 0.21683156490325928\n",
            "Epoch 160/200, Loss: 0.21057501435279846\n",
            "Epoch 170/200, Loss: 0.2048312872648239\n",
            "Epoch 180/200, Loss: 0.19953493773937225\n",
            "Epoch 190/200, Loss: 0.19463121891021729\n",
            "Epoch 200/200, Loss: 0.19007399678230286\n",
            "Epoch 10/200, Loss: 0.5971469879150391\n",
            "Epoch 20/200, Loss: 0.521569550037384\n",
            "Epoch 30/200, Loss: 0.4717772901058197\n",
            "Epoch 40/200, Loss: 0.4346831142902374\n",
            "Epoch 50/200, Loss: 0.4053221344947815\n",
            "Epoch 60/200, Loss: 0.3813595473766327\n",
            "Epoch 70/200, Loss: 0.36144009232521057\n",
            "Epoch 80/200, Loss: 0.3446585237979889\n",
            "Epoch 90/200, Loss: 0.3303622305393219\n",
            "Epoch 100/200, Loss: 0.3180619180202484\n",
            "Epoch 110/200, Loss: 0.3073827028274536\n",
            "Epoch 120/200, Loss: 0.2980332374572754\n",
            "Epoch 130/200, Loss: 0.2897844910621643\n",
            "Epoch 140/200, Loss: 0.2824550271034241\n",
            "Epoch 150/200, Loss: 0.2758994698524475\n",
            "Epoch 160/200, Loss: 0.27000054717063904\n",
            "Epoch 170/200, Loss: 0.2646629810333252\n",
            "Epoch 180/200, Loss: 0.2598085105419159\n",
            "Epoch 190/200, Loss: 0.2553725838661194\n",
            "Epoch 200/200, Loss: 0.25130146741867065\n",
            "Epoch 10/200, Loss: 0.5289151668548584\n",
            "Epoch 20/200, Loss: 0.46444380283355713\n",
            "Epoch 30/200, Loss: 0.4235491156578064\n",
            "Epoch 40/200, Loss: 0.3932367265224457\n",
            "Epoch 50/200, Loss: 0.36906149983406067\n",
            "Epoch 60/200, Loss: 0.3491009473800659\n",
            "Epoch 70/200, Loss: 0.33230066299438477\n",
            "Epoch 80/200, Loss: 0.3179744780063629\n",
            "Epoch 90/200, Loss: 0.30563053488731384\n",
            "Epoch 100/200, Loss: 0.2948979437351227\n",
            "Epoch 110/200, Loss: 0.28548961877822876\n",
            "Epoch 120/200, Loss: 0.27717965841293335\n",
            "Epoch 130/200, Loss: 0.2697884440422058\n",
            "Epoch 140/200, Loss: 0.26317161321640015\n",
            "Epoch 150/200, Loss: 0.25721240043640137\n",
            "Epoch 160/200, Loss: 0.251815527677536\n",
            "Epoch 170/200, Loss: 0.24690257012844086\n",
            "Epoch 180/200, Loss: 0.24240876734256744\n",
            "Epoch 190/200, Loss: 0.2382800579071045\n",
            "Epoch 200/200, Loss: 0.23447120189666748\n",
            "Epoch 10/200, Loss: 0.6961283087730408\n",
            "Epoch 20/200, Loss: 0.578959584236145\n",
            "Epoch 30/200, Loss: 0.5098583698272705\n",
            "Epoch 40/200, Loss: 0.46258968114852905\n",
            "Epoch 50/200, Loss: 0.4270854890346527\n",
            "Epoch 60/200, Loss: 0.39903366565704346\n",
            "Epoch 70/200, Loss: 0.3762222230434418\n",
            "Epoch 80/200, Loss: 0.3573192358016968\n",
            "Epoch 90/200, Loss: 0.3414301574230194\n",
            "Epoch 100/200, Loss: 0.3279145359992981\n",
            "Epoch 110/200, Loss: 0.3162963092327118\n",
            "Epoch 120/200, Loss: 0.30621346831321716\n",
            "Epoch 130/200, Loss: 0.2973865270614624\n",
            "Epoch 140/200, Loss: 0.2895966172218323\n",
            "Epoch 150/200, Loss: 0.2826709449291229\n",
            "Epoch 160/200, Loss: 0.2764713764190674\n",
            "Epoch 170/200, Loss: 0.27088674902915955\n",
            "Epoch 180/200, Loss: 0.2658267021179199\n",
            "Epoch 190/200, Loss: 0.26121723651885986\n",
            "Epoch 200/200, Loss: 0.25699734687805176\n",
            "Epoch 10/200, Loss: 0.6199905872344971\n",
            "Epoch 20/200, Loss: 0.5431504249572754\n",
            "Epoch 30/200, Loss: 0.4823218584060669\n",
            "Epoch 40/200, Loss: 0.43367114663124084\n",
            "Epoch 50/200, Loss: 0.3943054676055908\n",
            "Epoch 60/200, Loss: 0.3620651364326477\n",
            "Epoch 70/200, Loss: 0.33534082770347595\n",
            "Epoch 80/200, Loss: 0.31292957067489624\n",
            "Epoch 90/200, Loss: 0.29392629861831665\n",
            "Epoch 100/200, Loss: 0.27764490246772766\n",
            "Epoch 110/200, Loss: 0.26356029510498047\n",
            "Epoch 120/200, Loss: 0.2512669265270233\n",
            "Epoch 130/200, Loss: 0.2404482662677765\n",
            "Epoch 140/200, Loss: 0.2308550626039505\n",
            "Epoch 150/200, Loss: 0.22228915989398956\n",
            "Epoch 160/200, Loss: 0.21459147334098816\n",
            "Epoch 170/200, Loss: 0.20763325691223145\n",
            "Epoch 180/200, Loss: 0.20130950212478638\n",
            "Epoch 190/200, Loss: 0.19553369283676147\n",
            "Epoch 200/200, Loss: 0.19023427367210388\n",
            "Voting Accuracy: 0.125\n",
            "Weighted Voting Accuracy: 0.875\n",
            "Averaging Accuracy: 0.875\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "i = 10\n",
        "X_train_parts = np.array_split(X_train, i)\n",
        "y_train_parts = np.array_split(y_train, i)\n",
        "\n",
        "models = []\n",
        "for j in range(i):\n",
        "    model = MyLogisticRegression()\n",
        "    model.fit(X_train_parts[j], y_train_parts[j])\n",
        "    models.append(model)\n",
        "\n",
        "def voting_ensemble(models, X):\n",
        "    results = np.array([model.predict(X) for model in models])\n",
        "    return np.apply_along_axis(lambda x: np.argmax(np.bincount(x.astype(int))), axis=0, arr=results)\n",
        "\n",
        "def weighted_voting_ensemble(models, weights, X):\n",
        "    results = np.array([model.predict(X) for model in models])\n",
        "    weighted_results = np.average(results, axis=0, weights=weights)\n",
        "    return np.round(weighted_results).astype(int)\n",
        "\n",
        "def averaging_ensemble(models, X):\n",
        "    results = np.array([model.predict(X) for model in models])\n",
        "    return np.round(np.mean(results, axis=0)).astype(int)\n",
        "\n",
        "voting_predictions = voting_ensemble(models, X_test)\n",
        "weighted_voting_predictions = weighted_voting_ensemble(models, weights=None, X=X_test)\n",
        "averaging_predictions = averaging_ensemble(models, X_test)\n",
        "\n",
        "voting_accuracy = accuracy_score(y_test, voting_predictions)\n",
        "weighted_voting_accuracy = accuracy_score(y_test, weighted_voting_predictions)\n",
        "averaging_accuracy = accuracy_score(y_test, averaging_predictions)\n",
        "\n",
        "print(\"Voting Accuracy:\", voting_accuracy)\n",
        "print(\"Weighted Voting Accuracy:\", weighted_voting_accuracy)\n",
        "print(\"Averaging Accuracy:\", averaging_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QS9HYJ5FW1T"
      },
      "source": [
        "**Question:** Explain your proposed methods and the reason you decided to use them!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hCBQuAeF46a"
      },
      "source": [
        "**Your answer:**\n",
        "\n",
        "Voting Ensemble: Combines predictions by majority voting to make a final decision based on the most frequent class prediction. Helps reduce individual model biases and improve overall accuracy.\n",
        "\n",
        "Weighted Voting Ensemble: Assigns weights to model predictions and combines them using weighted voting. Useful when models have varying degrees of accuracy or reliability.\n",
        "\n",
        "Averaging Ensemble: Combines predictions by averaging probabilities or logits across models. Effective when models provide probability estimates or confidence scores for each class.\n",
        "\n",
        "These ensemble methods aim to leverage the collective intelligence of multiple models, mitigating individual weaknesses and enhancing overall prediction performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjSREvg4FTHf"
      },
      "source": [
        "**Task:** Now, for your best method, change $i$ from 2 to 100 and report $i$, train and test accuracy of the best model. Also, plot test and train accuracy for $2\\leq i\\leq100$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfKS-Jq0-v4P",
        "outputId": "3b1a856e-8a5d-4918-88ee-2d2106af9202"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/50, Loss: 0.32200494408607483\n",
            "Epoch 20/50, Loss: 0.3237045407295227\n",
            "Epoch 30/50, Loss: 0.11136426031589508\n",
            "Epoch 40/50, Loss: 0.296258807182312\n",
            "Epoch 50/50, Loss: 0.22524578869342804\n",
            "Epoch 10/50, Loss: 0.33972784876823425\n",
            "Epoch 20/50, Loss: 0.1812022477388382\n",
            "Epoch 30/50, Loss: 0.22211456298828125\n",
            "Epoch 40/50, Loss: 0.19920851290225983\n",
            "Epoch 50/50, Loss: 0.2144821584224701\n",
            "Epoch 10/50, Loss: 0.27308619022369385\n",
            "Epoch 20/50, Loss: 0.17106616497039795\n",
            "Epoch 30/50, Loss: 0.2450307011604309\n",
            "Epoch 40/50, Loss: 0.2984449863433838\n",
            "Epoch 50/50, Loss: 0.29025715589523315\n",
            "Epoch 10/50, Loss: 0.2973611354827881\n",
            "Epoch 20/50, Loss: 0.2986970543861389\n",
            "Epoch 30/50, Loss: 0.2737152576446533\n",
            "Epoch 40/50, Loss: 0.14469313621520996\n",
            "Epoch 50/50, Loss: 0.2466650754213333\n",
            "Epoch 10/50, Loss: 0.3034728169441223\n",
            "Epoch 20/50, Loss: 0.2812255918979645\n",
            "Epoch 30/50, Loss: 0.3442821502685547\n",
            "Epoch 40/50, Loss: 0.1659342497587204\n",
            "Epoch 50/50, Loss: 0.13300490379333496\n",
            "Epoch 10/50, Loss: 0.36110806465148926\n",
            "Epoch 20/50, Loss: 0.20525787770748138\n",
            "Epoch 30/50, Loss: 0.28567951917648315\n",
            "Epoch 40/50, Loss: 0.1782360076904297\n",
            "Epoch 50/50, Loss: 0.14056581258773804\n",
            "Epoch 10/50, Loss: 0.3130525052547455\n",
            "Epoch 20/50, Loss: 0.2629214823246002\n",
            "Epoch 30/50, Loss: 0.23255370557308197\n",
            "Epoch 40/50, Loss: 0.18003208935260773\n",
            "Epoch 50/50, Loss: 0.1750311553478241\n",
            "Epoch 10/50, Loss: 0.3868146538734436\n",
            "Epoch 20/50, Loss: 0.24605309963226318\n",
            "Epoch 30/50, Loss: 0.22038991749286652\n",
            "Epoch 40/50, Loss: 0.14522135257720947\n",
            "Epoch 50/50, Loss: 0.1492435783147812\n",
            "Epoch 10/50, Loss: 0.27228957414627075\n",
            "Epoch 20/50, Loss: 0.23659799993038177\n",
            "Epoch 30/50, Loss: 0.20085695385932922\n",
            "Epoch 40/50, Loss: 0.2037891000509262\n",
            "Epoch 50/50, Loss: 0.14225846529006958\n",
            "Epoch 10/50, Loss: 0.26936501264572144\n",
            "Epoch 20/50, Loss: 0.25328800082206726\n",
            "Epoch 30/50, Loss: 0.21571625769138336\n",
            "Epoch 40/50, Loss: 0.25042811036109924\n",
            "Epoch 50/50, Loss: 0.23657596111297607\n",
            "Epoch 10/50, Loss: 0.2591412663459778\n",
            "Epoch 20/50, Loss: 0.17891231179237366\n",
            "Epoch 30/50, Loss: 0.15643788874149323\n",
            "Epoch 40/50, Loss: 0.26498621702194214\n",
            "Epoch 50/50, Loss: 0.12458166480064392\n",
            "Epoch 10/50, Loss: 0.31820452213287354\n",
            "Epoch 20/50, Loss: 0.2516334056854248\n",
            "Epoch 30/50, Loss: 0.2537263333797455\n",
            "Epoch 40/50, Loss: 0.14584477245807648\n",
            "Epoch 50/50, Loss: 0.09776794910430908\n",
            "Epoch 10/50, Loss: 0.3039568364620209\n",
            "Epoch 20/50, Loss: 0.223459392786026\n",
            "Epoch 30/50, Loss: 0.2079428881406784\n",
            "Epoch 40/50, Loss: 0.16541366279125214\n",
            "Epoch 50/50, Loss: 0.17677505314350128\n",
            "Epoch 10/50, Loss: 0.20535477995872498\n",
            "Epoch 20/50, Loss: 0.2873990833759308\n",
            "Epoch 30/50, Loss: 0.15997597575187683\n",
            "Epoch 40/50, Loss: 0.1443517804145813\n",
            "Epoch 50/50, Loss: 0.13358746469020844\n",
            "Epoch 10/50, Loss: 0.26472213864326477\n",
            "Epoch 20/50, Loss: 0.2531232237815857\n",
            "Epoch 30/50, Loss: 0.22648753225803375\n",
            "Epoch 40/50, Loss: 0.23143847286701202\n",
            "Epoch 50/50, Loss: 0.275177538394928\n",
            "Epoch 10/50, Loss: 0.30243855714797974\n",
            "Epoch 20/50, Loss: 0.17308920621871948\n",
            "Epoch 30/50, Loss: 0.2768007814884186\n",
            "Epoch 40/50, Loss: 0.26958730816841125\n",
            "Epoch 50/50, Loss: 0.17811892926692963\n",
            "Epoch 10/50, Loss: 0.32963675260543823\n",
            "Epoch 20/50, Loss: 0.24236106872558594\n",
            "Epoch 30/50, Loss: 0.19083654880523682\n",
            "Epoch 40/50, Loss: 0.22562378644943237\n",
            "Epoch 50/50, Loss: 0.25208780169487\n",
            "Epoch 10/50, Loss: 0.2916657030582428\n",
            "Epoch 20/50, Loss: 0.24671921133995056\n",
            "Epoch 30/50, Loss: 0.19148297607898712\n",
            "Epoch 40/50, Loss: 0.21390773355960846\n",
            "Epoch 50/50, Loss: 0.21553222835063934\n",
            "Epoch 10/50, Loss: 0.2877362370491028\n",
            "Epoch 20/50, Loss: 0.20983408391475677\n",
            "Epoch 30/50, Loss: 0.20237238705158234\n",
            "Epoch 40/50, Loss: 0.16802838444709778\n",
            "Epoch 50/50, Loss: 0.16583307087421417\n",
            "Epoch 10/50, Loss: 0.3295169770717621\n",
            "Epoch 20/50, Loss: 0.19996410608291626\n",
            "Epoch 30/50, Loss: 0.21882657706737518\n",
            "Epoch 40/50, Loss: 0.2385099232196808\n",
            "Epoch 50/50, Loss: 0.21222859621047974\n",
            "Epoch 10/50, Loss: 0.27288657426834106\n",
            "Epoch 20/50, Loss: 0.16386504471302032\n",
            "Epoch 30/50, Loss: 0.2109486162662506\n",
            "Epoch 40/50, Loss: 0.09423625469207764\n",
            "Epoch 50/50, Loss: 0.07459213584661484\n",
            "Epoch 10/50, Loss: 0.3012295663356781\n",
            "Epoch 20/50, Loss: 0.22025099396705627\n",
            "Epoch 30/50, Loss: 0.16875949501991272\n",
            "Epoch 40/50, Loss: 0.22339418530464172\n",
            "Epoch 50/50, Loss: 0.26271873712539673\n",
            "Epoch 10/50, Loss: 0.3243682384490967\n",
            "Epoch 20/50, Loss: 0.2296498864889145\n",
            "Epoch 30/50, Loss: 0.16684524714946747\n",
            "Epoch 40/50, Loss: 0.1244862973690033\n",
            "Epoch 50/50, Loss: 0.1624075323343277\n",
            "Epoch 10/50, Loss: 0.36459654569625854\n",
            "Epoch 20/50, Loss: 0.24753320217132568\n",
            "Epoch 30/50, Loss: 0.25079935789108276\n",
            "Epoch 40/50, Loss: 0.156620591878891\n",
            "Epoch 50/50, Loss: 0.2503246068954468\n",
            "Epoch 10/50, Loss: 0.31664127111434937\n",
            "Epoch 20/50, Loss: 0.28301310539245605\n",
            "Epoch 30/50, Loss: 0.09563907235860825\n",
            "Epoch 40/50, Loss: 0.22497199475765228\n",
            "Epoch 50/50, Loss: 0.15897506475448608\n",
            "Epoch 10/50, Loss: 0.323303759098053\n",
            "Epoch 20/50, Loss: 0.3255573809146881\n",
            "Epoch 30/50, Loss: 0.14887835085391998\n",
            "Epoch 40/50, Loss: 0.16784541308879852\n",
            "Epoch 50/50, Loss: 0.14258427917957306\n",
            "Epoch 10/50, Loss: 0.34646812081336975\n",
            "Epoch 20/50, Loss: 0.33435407280921936\n",
            "Epoch 30/50, Loss: 0.2144245058298111\n",
            "Epoch 40/50, Loss: 0.2008221298456192\n",
            "Epoch 50/50, Loss: 0.19724668562412262\n",
            "Epoch 10/50, Loss: 0.27097001671791077\n",
            "Epoch 20/50, Loss: 0.24686020612716675\n",
            "Epoch 30/50, Loss: 0.20805120468139648\n",
            "Epoch 40/50, Loss: 0.2156277447938919\n",
            "Epoch 50/50, Loss: 0.20524390041828156\n",
            "Epoch 10/50, Loss: 0.3004486560821533\n",
            "Epoch 20/50, Loss: 0.3337814211845398\n",
            "Epoch 30/50, Loss: 0.1516546905040741\n",
            "Epoch 40/50, Loss: 0.23723027110099792\n",
            "Epoch 50/50, Loss: 0.32301655411720276\n",
            "Epoch 10/50, Loss: 0.276203453540802\n",
            "Epoch 20/50, Loss: 0.28147339820861816\n",
            "Epoch 30/50, Loss: 0.240820050239563\n",
            "Epoch 40/50, Loss: 0.24203184247016907\n",
            "Epoch 50/50, Loss: 0.2849121689796448\n",
            "Epoch 10/50, Loss: 0.34131595492362976\n",
            "Epoch 20/50, Loss: 0.2622494101524353\n",
            "Epoch 30/50, Loss: 0.1429816037416458\n",
            "Epoch 40/50, Loss: 0.23735590279102325\n",
            "Epoch 50/50, Loss: 0.28170135617256165\n",
            "Epoch 10/50, Loss: 0.31002527475357056\n",
            "Epoch 20/50, Loss: 0.30550849437713623\n",
            "Epoch 30/50, Loss: 0.17515072226524353\n",
            "Epoch 40/50, Loss: 0.2030383199453354\n",
            "Epoch 50/50, Loss: 0.18217091262340546\n",
            "Epoch 10/50, Loss: 0.25768551230430603\n",
            "Epoch 20/50, Loss: 0.3767910897731781\n",
            "Epoch 30/50, Loss: 0.17109480500221252\n",
            "Epoch 40/50, Loss: 0.2529696524143219\n",
            "Epoch 50/50, Loss: 0.2729414701461792\n",
            "Epoch 10/50, Loss: 0.31309181451797485\n",
            "Epoch 20/50, Loss: 0.35430479049682617\n",
            "Epoch 30/50, Loss: 0.1535928100347519\n",
            "Epoch 40/50, Loss: 0.16233724355697632\n",
            "Epoch 50/50, Loss: 0.18500712513923645\n",
            "Epoch 10/50, Loss: 0.24306590855121613\n",
            "Epoch 20/50, Loss: 0.3000224530696869\n",
            "Epoch 30/50, Loss: 0.27321624755859375\n",
            "Epoch 40/50, Loss: 0.12576378881931305\n",
            "Epoch 50/50, Loss: 0.14931578934192657\n",
            "Epoch 10/50, Loss: 0.29723089933395386\n",
            "Epoch 20/50, Loss: 0.20576493442058563\n",
            "Epoch 30/50, Loss: 0.4009844660758972\n",
            "Epoch 40/50, Loss: 0.15869459509849548\n",
            "Epoch 50/50, Loss: 0.2122902274131775\n",
            "Epoch 10/50, Loss: 0.2745470702648163\n",
            "Epoch 20/50, Loss: 0.20228786766529083\n",
            "Epoch 30/50, Loss: 0.219496488571167\n",
            "Epoch 40/50, Loss: 0.3017374873161316\n",
            "Epoch 50/50, Loss: 0.20072148740291595\n",
            "Epoch 10/50, Loss: 0.30933961272239685\n",
            "Epoch 20/50, Loss: 0.256130576133728\n",
            "Epoch 30/50, Loss: 0.21448606252670288\n",
            "Epoch 40/50, Loss: 0.2042044848203659\n",
            "Epoch 50/50, Loss: 0.24597413837909698\n",
            "Epoch 10/50, Loss: 0.2959677278995514\n",
            "Epoch 20/50, Loss: 0.19658038020133972\n",
            "Epoch 30/50, Loss: 0.24202598631381989\n",
            "Epoch 40/50, Loss: 0.2406548112630844\n",
            "Epoch 50/50, Loss: 0.13332518935203552\n",
            "Epoch 10/50, Loss: 0.21674925088882446\n",
            "Epoch 20/50, Loss: 0.2342698723077774\n",
            "Epoch 30/50, Loss: 0.25004544854164124\n",
            "Epoch 40/50, Loss: 0.20507992804050446\n",
            "Epoch 50/50, Loss: 0.0692572072148323\n",
            "Epoch 10/50, Loss: 0.36457014083862305\n",
            "Epoch 20/50, Loss: 0.21494020521640778\n",
            "Epoch 30/50, Loss: 0.16763612627983093\n",
            "Epoch 40/50, Loss: 0.2005385011434555\n",
            "Epoch 50/50, Loss: 0.14723099768161774\n",
            "Epoch 10/50, Loss: 0.3308697044849396\n",
            "Epoch 20/50, Loss: 0.1524781882762909\n",
            "Epoch 30/50, Loss: 0.19524291157722473\n",
            "Epoch 40/50, Loss: 0.1972714513540268\n",
            "Epoch 50/50, Loss: 0.12449231743812561\n",
            "Epoch 10/50, Loss: 0.2970867156982422\n",
            "Epoch 20/50, Loss: 0.2299310714006424\n",
            "Epoch 30/50, Loss: 0.3002736568450928\n",
            "Epoch 40/50, Loss: 0.11078518629074097\n",
            "Epoch 50/50, Loss: 0.11264634877443314\n",
            "Epoch 10/50, Loss: 0.27605384588241577\n",
            "Epoch 20/50, Loss: 0.22757689654827118\n",
            "Epoch 30/50, Loss: 0.23495431244373322\n",
            "Epoch 40/50, Loss: 0.19437503814697266\n",
            "Epoch 50/50, Loss: 0.18884038925170898\n",
            "Epoch 10/50, Loss: 0.3729873299598694\n",
            "Epoch 20/50, Loss: 0.1974775642156601\n",
            "Epoch 30/50, Loss: 0.32980406284332275\n",
            "Epoch 40/50, Loss: 0.21594098210334778\n",
            "Epoch 50/50, Loss: 0.25635501742362976\n",
            "Epoch 10/50, Loss: 0.3923523426055908\n",
            "Epoch 20/50, Loss: 0.24242976307868958\n",
            "Epoch 30/50, Loss: 0.24462082982063293\n",
            "Epoch 40/50, Loss: 0.11005046218633652\n",
            "Epoch 50/50, Loss: 0.16425593197345734\n",
            "Epoch 10/50, Loss: 0.27803120017051697\n",
            "Epoch 20/50, Loss: 0.2112215757369995\n",
            "Epoch 30/50, Loss: 0.23834455013275146\n",
            "Epoch 40/50, Loss: 0.2566748857498169\n",
            "Epoch 50/50, Loss: 0.2060544788837433\n",
            "Epoch 10/50, Loss: 0.299447238445282\n",
            "Epoch 20/50, Loss: 0.22842147946357727\n",
            "Epoch 30/50, Loss: 0.20574332773685455\n",
            "Epoch 40/50, Loss: 0.31800150871276855\n",
            "Epoch 50/50, Loss: 0.08570823818445206\n",
            "Epoch 10/50, Loss: 0.3707471489906311\n",
            "Epoch 20/50, Loss: 0.2348622828722\n",
            "Epoch 30/50, Loss: 0.16206538677215576\n",
            "Epoch 40/50, Loss: 0.19930818676948547\n",
            "Epoch 50/50, Loss: 0.1987236887216568\n",
            "Epoch 10/50, Loss: 0.31605207920074463\n",
            "Epoch 20/50, Loss: 0.18128494918346405\n",
            "Epoch 30/50, Loss: 0.2518254816532135\n",
            "Epoch 40/50, Loss: 0.08742284774780273\n",
            "Epoch 50/50, Loss: 0.2075624316930771\n",
            "Epoch 10/50, Loss: 0.31728434562683105\n",
            "Epoch 20/50, Loss: 0.26233741641044617\n",
            "Epoch 30/50, Loss: 0.20237858593463898\n",
            "Epoch 40/50, Loss: 0.24686121940612793\n",
            "Epoch 50/50, Loss: 0.18611401319503784\n",
            "Epoch 10/50, Loss: 0.26934051513671875\n",
            "Epoch 20/50, Loss: 0.17921856045722961\n",
            "Epoch 30/50, Loss: 0.2234562635421753\n",
            "Epoch 40/50, Loss: 0.16025933623313904\n",
            "Epoch 50/50, Loss: 0.19716453552246094\n",
            "Epoch 10/50, Loss: 0.34112370014190674\n",
            "Epoch 20/50, Loss: 0.2975773215293884\n",
            "Epoch 30/50, Loss: 0.1375804841518402\n",
            "Epoch 40/50, Loss: 0.21403488516807556\n",
            "Epoch 50/50, Loss: 0.14478124678134918\n",
            "Epoch 10/50, Loss: 0.35821086168289185\n",
            "Epoch 20/50, Loss: 0.21408188343048096\n",
            "Epoch 30/50, Loss: 0.17531225085258484\n",
            "Epoch 40/50, Loss: 0.1547875851392746\n",
            "Epoch 50/50, Loss: 0.12170281261205673\n",
            "Epoch 10/50, Loss: 0.3277978301048279\n",
            "Epoch 20/50, Loss: 0.30230942368507385\n",
            "Epoch 30/50, Loss: 0.3051781952381134\n",
            "Epoch 40/50, Loss: 0.24449385702610016\n",
            "Epoch 50/50, Loss: 0.21964888274669647\n",
            "Epoch 10/50, Loss: 0.30308011174201965\n",
            "Epoch 20/50, Loss: 0.27996325492858887\n",
            "Epoch 30/50, Loss: 0.22861018776893616\n",
            "Epoch 40/50, Loss: 0.14581555128097534\n",
            "Epoch 50/50, Loss: 0.12409570813179016\n",
            "Epoch 10/50, Loss: 0.34686028957366943\n",
            "Epoch 20/50, Loss: 0.15346600115299225\n",
            "Epoch 30/50, Loss: 0.24287788569927216\n",
            "Epoch 40/50, Loss: 0.2300407588481903\n",
            "Epoch 50/50, Loss: 0.13449200987815857\n",
            "Epoch 10/50, Loss: 0.2962382435798645\n",
            "Epoch 20/50, Loss: 0.23335352540016174\n",
            "Epoch 30/50, Loss: 0.15962505340576172\n",
            "Epoch 40/50, Loss: 0.14638911187648773\n",
            "Epoch 50/50, Loss: 0.12822040915489197\n",
            "Epoch 10/50, Loss: 0.36019378900527954\n",
            "Epoch 20/50, Loss: 0.24639546871185303\n",
            "Epoch 30/50, Loss: 0.1827477514743805\n",
            "Epoch 40/50, Loss: 0.11810677498579025\n",
            "Epoch 50/50, Loss: 0.17899712920188904\n",
            "Epoch 10/50, Loss: 0.2916635274887085\n",
            "Epoch 20/50, Loss: 0.18986523151397705\n",
            "Epoch 30/50, Loss: 0.23672740161418915\n",
            "Epoch 40/50, Loss: 0.21284684538841248\n",
            "Epoch 50/50, Loss: 0.14326143264770508\n",
            "Epoch 10/50, Loss: 0.4149675965309143\n",
            "Epoch 20/50, Loss: 0.22614464163780212\n",
            "Epoch 30/50, Loss: 0.22799420356750488\n",
            "Epoch 40/50, Loss: 0.2536669373512268\n",
            "Epoch 50/50, Loss: 0.10937588661909103\n",
            "Epoch 10/50, Loss: 0.2433520406484604\n",
            "Epoch 20/50, Loss: 0.2780528962612152\n",
            "Epoch 30/50, Loss: 0.22066761553287506\n",
            "Epoch 40/50, Loss: 0.11682501435279846\n",
            "Epoch 50/50, Loss: 0.31897175312042236\n",
            "Epoch 10/50, Loss: 0.28683701157569885\n",
            "Epoch 20/50, Loss: 0.23047129809856415\n",
            "Epoch 30/50, Loss: 0.11762979626655579\n",
            "Epoch 40/50, Loss: 0.28191477060317993\n",
            "Epoch 50/50, Loss: 0.14656195044517517\n",
            "Epoch 10/50, Loss: 0.32053616642951965\n",
            "Epoch 20/50, Loss: 0.24021010100841522\n",
            "Epoch 30/50, Loss: 0.27348023653030396\n",
            "Epoch 40/50, Loss: 0.3596252202987671\n",
            "Epoch 50/50, Loss: 0.18635202944278717\n",
            "Epoch 10/50, Loss: 0.22685816884040833\n",
            "Epoch 20/50, Loss: 0.3192318379878998\n",
            "Epoch 30/50, Loss: 0.244105264544487\n",
            "Epoch 40/50, Loss: 0.21931537985801697\n",
            "Epoch 50/50, Loss: 0.2026466280221939\n",
            "Epoch 10/50, Loss: 0.26730215549468994\n",
            "Epoch 20/50, Loss: 0.19822724163532257\n",
            "Epoch 30/50, Loss: 0.14397425949573517\n",
            "Epoch 40/50, Loss: 0.14713042974472046\n",
            "Epoch 50/50, Loss: 0.11692069470882416\n",
            "Epoch 10/50, Loss: 0.3087468147277832\n",
            "Epoch 20/50, Loss: 0.2799152731895447\n",
            "Epoch 30/50, Loss: 0.35359296202659607\n",
            "Epoch 40/50, Loss: 0.29033875465393066\n",
            "Epoch 50/50, Loss: 0.12549249827861786\n",
            "Epoch 10/50, Loss: 0.28651663661003113\n",
            "Epoch 20/50, Loss: 0.21688872575759888\n",
            "Epoch 30/50, Loss: 0.2595365643501282\n",
            "Epoch 40/50, Loss: 0.20354710519313812\n",
            "Epoch 50/50, Loss: 0.20457929372787476\n",
            "Epoch 10/50, Loss: 0.255771666765213\n",
            "Epoch 20/50, Loss: 0.32035279273986816\n",
            "Epoch 30/50, Loss: 0.23190759122371674\n",
            "Epoch 40/50, Loss: 0.1632624864578247\n",
            "Epoch 50/50, Loss: 0.14844724535942078\n",
            "Epoch 10/50, Loss: 0.3732032775878906\n",
            "Epoch 20/50, Loss: 0.25878316164016724\n",
            "Epoch 30/50, Loss: 0.13087911903858185\n",
            "Epoch 40/50, Loss: 0.3024049699306488\n",
            "Epoch 50/50, Loss: 0.20559880137443542\n",
            "Epoch 10/50, Loss: 0.3041510283946991\n",
            "Epoch 20/50, Loss: 0.2612549066543579\n",
            "Epoch 30/50, Loss: 0.12008974701166153\n",
            "Epoch 40/50, Loss: 0.2558860778808594\n",
            "Epoch 50/50, Loss: 0.11100485920906067\n",
            "Epoch 10/50, Loss: 0.37091779708862305\n",
            "Epoch 20/50, Loss: 0.24773311614990234\n",
            "Epoch 30/50, Loss: 0.2195294350385666\n",
            "Epoch 40/50, Loss: 0.29814234375953674\n",
            "Epoch 50/50, Loss: 0.20251038670539856\n",
            "Epoch 10/50, Loss: 0.3353974521160126\n",
            "Epoch 20/50, Loss: 0.24225640296936035\n",
            "Epoch 30/50, Loss: 0.26062411069869995\n",
            "Epoch 40/50, Loss: 0.2530151307582855\n",
            "Epoch 50/50, Loss: 0.2590517997741699\n",
            "Epoch 10/50, Loss: 0.2805388271808624\n",
            "Epoch 20/50, Loss: 0.21424144506454468\n",
            "Epoch 30/50, Loss: 0.16192246973514557\n",
            "Epoch 40/50, Loss: 0.15957492589950562\n",
            "Epoch 50/50, Loss: 0.14257310330867767\n",
            "Epoch 10/50, Loss: 0.33564257621765137\n",
            "Epoch 20/50, Loss: 0.21597512066364288\n",
            "Epoch 30/50, Loss: 0.15695559978485107\n",
            "Epoch 40/50, Loss: 0.1364789605140686\n",
            "Epoch 50/50, Loss: 0.2078707367181778\n",
            "Epoch 10/50, Loss: 0.3198546767234802\n",
            "Epoch 20/50, Loss: 0.305097758769989\n",
            "Epoch 30/50, Loss: 0.1672438383102417\n",
            "Epoch 40/50, Loss: 0.16350819170475006\n",
            "Epoch 50/50, Loss: 0.22106564044952393\n",
            "Epoch 10/50, Loss: 0.27592581510543823\n",
            "Epoch 20/50, Loss: 0.20672821998596191\n",
            "Epoch 30/50, Loss: 0.24806281924247742\n",
            "Epoch 40/50, Loss: 0.10053970664739609\n",
            "Epoch 50/50, Loss: 0.12255214154720306\n",
            "Epoch 10/50, Loss: 0.3493562638759613\n",
            "Epoch 20/50, Loss: 0.2660248577594757\n",
            "Epoch 30/50, Loss: 0.22528515756130219\n",
            "Epoch 40/50, Loss: 0.25302812457084656\n",
            "Epoch 50/50, Loss: 0.13854031264781952\n",
            "Epoch 10/50, Loss: 0.42092031240463257\n",
            "Epoch 20/50, Loss: 0.2670360505580902\n",
            "Epoch 30/50, Loss: 0.30530834197998047\n",
            "Epoch 40/50, Loss: 0.12756562232971191\n",
            "Epoch 50/50, Loss: 0.07868794351816177\n",
            "Epoch 10/50, Loss: 0.2786630094051361\n",
            "Epoch 20/50, Loss: 0.14367438852787018\n",
            "Epoch 30/50, Loss: 0.19291138648986816\n",
            "Epoch 40/50, Loss: 0.20431818068027496\n",
            "Epoch 50/50, Loss: 0.14598281681537628\n",
            "Epoch 10/50, Loss: 0.3204702138900757\n",
            "Epoch 20/50, Loss: 0.18942038714885712\n",
            "Epoch 30/50, Loss: 0.2071441411972046\n",
            "Epoch 40/50, Loss: 0.07851960510015488\n",
            "Epoch 50/50, Loss: 0.2998207211494446\n",
            "Epoch 10/50, Loss: 0.27934888005256653\n",
            "Epoch 20/50, Loss: 0.22465521097183228\n",
            "Epoch 30/50, Loss: 0.2440345138311386\n",
            "Epoch 40/50, Loss: 0.1362241953611374\n",
            "Epoch 50/50, Loss: 0.1677473783493042\n",
            "Epoch 10/50, Loss: 0.2787293791770935\n",
            "Epoch 20/50, Loss: 0.23440402746200562\n",
            "Epoch 30/50, Loss: 0.16972637176513672\n",
            "Epoch 40/50, Loss: 0.19314634799957275\n",
            "Epoch 50/50, Loss: 0.22428658604621887\n",
            "Epoch 10/50, Loss: 0.3376676142215729\n",
            "Epoch 20/50, Loss: 0.2746568024158478\n",
            "Epoch 30/50, Loss: 0.24315524101257324\n",
            "Epoch 40/50, Loss: 0.09304206818342209\n",
            "Epoch 50/50, Loss: 0.15928781032562256\n",
            "Epoch 10/50, Loss: 0.2789459824562073\n",
            "Epoch 20/50, Loss: 0.2683772146701813\n",
            "Epoch 30/50, Loss: 0.20165738463401794\n",
            "Epoch 40/50, Loss: 0.3633022606372833\n",
            "Epoch 50/50, Loss: 0.18608634173870087\n",
            "Epoch 10/50, Loss: 0.3876909613609314\n",
            "Epoch 20/50, Loss: 0.17296208441257477\n",
            "Epoch 30/50, Loss: 0.1231156662106514\n",
            "Epoch 40/50, Loss: 0.21430084109306335\n",
            "Epoch 50/50, Loss: 0.28181540966033936\n",
            "Epoch 10/50, Loss: 0.37473079562187195\n",
            "Epoch 20/50, Loss: 0.21407286822795868\n",
            "Epoch 30/50, Loss: 0.21001841127872467\n",
            "Epoch 40/50, Loss: 0.34186822175979614\n",
            "Epoch 50/50, Loss: 0.19979308545589447\n",
            "Epoch 10/50, Loss: 0.26320773363113403\n",
            "Epoch 20/50, Loss: 0.2980714738368988\n",
            "Epoch 30/50, Loss: 0.21056203544139862\n",
            "Epoch 40/50, Loss: 0.20828594267368317\n",
            "Epoch 50/50, Loss: 0.18181054294109344\n",
            "Epoch 10/50, Loss: 0.34204667806625366\n",
            "Epoch 20/50, Loss: 0.18699665367603302\n",
            "Epoch 30/50, Loss: 0.2030114233493805\n",
            "Epoch 40/50, Loss: 0.11723282933235168\n",
            "Epoch 50/50, Loss: 0.26145243644714355\n",
            "Epoch 10/50, Loss: 0.32764068245887756\n",
            "Epoch 20/50, Loss: 0.2598040699958801\n",
            "Epoch 30/50, Loss: 0.21033704280853271\n",
            "Epoch 40/50, Loss: 0.16303682327270508\n",
            "Epoch 50/50, Loss: 0.20862968266010284\n",
            "Epoch 10/50, Loss: 0.32043883204460144\n",
            "Epoch 20/50, Loss: 0.2128995656967163\n",
            "Epoch 30/50, Loss: 0.19878344237804413\n",
            "Epoch 40/50, Loss: 0.23051829636096954\n",
            "Epoch 50/50, Loss: 0.1623680740594864\n",
            "Epoch 10/50, Loss: 0.3625892698764801\n",
            "Epoch 20/50, Loss: 0.30511221289634705\n",
            "Epoch 30/50, Loss: 0.15307310223579407\n",
            "Epoch 40/50, Loss: 0.26173335313796997\n",
            "Epoch 50/50, Loss: 0.20249181985855103\n",
            "Epoch 10/50, Loss: 0.30892378091812134\n",
            "Epoch 20/50, Loss: 0.22641153633594513\n",
            "Epoch 30/50, Loss: 0.2567422091960907\n",
            "Epoch 40/50, Loss: 0.16236098110675812\n",
            "Epoch 50/50, Loss: 0.21593016386032104\n",
            "Epoch 10/50, Loss: 0.2947370707988739\n",
            "Epoch 20/50, Loss: 0.20605632662773132\n",
            "Epoch 30/50, Loss: 0.36072033643722534\n",
            "Epoch 40/50, Loss: 0.3616845905780792\n",
            "Epoch 50/50, Loss: 0.21638086438179016\n",
            "Epoch 10/50, Loss: 0.2979239225387573\n",
            "Epoch 20/50, Loss: 0.17522954940795898\n",
            "Epoch 30/50, Loss: 0.19148403406143188\n",
            "Epoch 40/50, Loss: 0.2115839272737503\n",
            "Epoch 50/50, Loss: 0.13366667926311493\n",
            "Epoch 10/50, Loss: 0.3446226716041565\n",
            "Epoch 20/50, Loss: 0.2686435878276825\n",
            "Epoch 30/50, Loss: 0.30439257621765137\n",
            "Epoch 40/50, Loss: 0.1229691132903099\n",
            "Epoch 50/50, Loss: 0.22581759095191956\n",
            "Epoch 10/50, Loss: 0.3793102204799652\n",
            "Epoch 20/50, Loss: 0.23953984677791595\n",
            "Epoch 30/50, Loss: 0.22152455151081085\n",
            "Epoch 40/50, Loss: 0.14564083516597748\n",
            "Epoch 50/50, Loss: 0.11990092694759369\n",
            "Epoch 10/50, Loss: 0.3525981307029724\n",
            "Epoch 20/50, Loss: 0.23934760689735413\n",
            "Epoch 30/50, Loss: 0.15734653174877167\n",
            "Epoch 40/50, Loss: 0.2779303789138794\n",
            "Epoch 50/50, Loss: 0.11409203708171844\n",
            "Epoch 10/50, Loss: 0.3192671537399292\n",
            "Epoch 20/50, Loss: 0.36022040247917175\n",
            "Epoch 30/50, Loss: 0.1909252107143402\n",
            "Epoch 40/50, Loss: 0.10385197401046753\n",
            "Epoch 50/50, Loss: 0.07914220541715622\n",
            "Best Model:\n",
            "i: 22\n",
            "Train Accuracy: 0.940625\n",
            "Test Accuracy: 0.925\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "best_model = None\n",
        "best_test_accuracy = 0.0\n",
        "train_accuracies = []\n",
        "test_accuracies = []\n",
        "\n",
        "for i in range(2, 101):\n",
        "    model = MyLogisticRegression(num_epochs=50, learning_rate=0.05, batch_size=32)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_train_pred_binary = np.where(y_train_pred > 0.5, 1, 0)\n",
        "\n",
        "    y_test_pred = model.predict(X_test)\n",
        "    y_test_pred_binary = np.where(y_test_pred > 0.5, 1, 0)\n",
        "\n",
        "    train_accuracy = accuracy_score(y_train, y_train_pred_binary)\n",
        "    test_accuracy = accuracy_score(y_test, y_test_pred_binary)\n",
        "\n",
        "\n",
        "    train_accuracies.append(train_accuracy)\n",
        "    test_accuracies.append(test_accuracy)\n",
        "\n",
        "    if test_accuracy > best_test_accuracy:\n",
        "        best_model = model\n",
        "        best_test_accuracy = test_accuracy\n",
        "\n",
        "best_model_index = test_accuracies.index(best_test_accuracy)\n",
        "best_model_i = best_model_index + 2\n",
        "\n",
        "print(\"Best Model:\")\n",
        "print(f\"i: {best_model_i}\")\n",
        "print(f\"Train Accuracy: {train_accuracies[best_model_index]}\")\n",
        "print(f\"Test Accuracy: {best_test_accuracy}\")\n",
        "\n",
        "# Plotting the accuracies\n",
        "x = range(2, 101)\n",
        "plt.plot(x, train_accuracies, label='Train Accuracy')\n",
        "plt.plot(x, test_accuracies, label='Test Accuracy')\n",
        "plt.xlabel('i')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWV0YUgRGg1p"
      },
      "source": [
        "**Question:** Analyze the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYjAT0joMCZ8"
      },
      "source": [
        "**Your Answer:** it is better to have bigger training part but in this case the difference is not much"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}