{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oc2CPMNF8y5d"
   },
   "source": [
    "<h1 align=\"center\">Introduction to Machine Learning - 25737-2</h1>\n",
    "<h4 align=\"center\">Dr. R. Amiri</h4>\n",
    "<h4 align=\"center\">Sharif University of Technology, Summer 2024</h4>\n",
    "<h4 align=\"center\">Project Phase one</h4>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student Name**: Zahra Maleki & Hossain Anjidani\n",
    "\n",
    "**Student ID**: 400110009 & 400100746"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T20:27:16.159407Z",
     "iopub.status.busy": "2024-07-01T20:27:16.159092Z",
     "iopub.status.idle": "2024-07-01T20:27:22.433691Z",
     "shell.execute_reply": "2024-07-01T20:27:22.432893Z",
     "shell.execute_reply.started": "2024-07-01T20:27:16.159378Z"
    },
    "id": "DfEgCm4K8y5f"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Subset\n",
    "import random\n",
    "import time\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T20:28:06.133504Z",
     "iopub.status.busy": "2024-07-01T20:28:06.132779Z",
     "iopub.status.idle": "2024-07-01T20:28:06.191086Z",
     "shell.execute_reply": "2024-07-01T20:28:06.190033Z",
     "shell.execute_reply.started": "2024-07-01T20:28:06.133472Z"
    },
    "id": "fFaABaXvpM2N"
   },
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZNRbSqY68y5f"
   },
   "source": [
    "**Simulation Question 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T20:28:10.131223Z",
     "iopub.status.busy": "2024-07-01T20:28:10.130513Z",
     "iopub.status.idle": "2024-07-01T20:28:10.138081Z",
     "shell.execute_reply": "2024-07-01T20:28:10.137063Z",
     "shell.execute_reply.started": "2024-07-01T20:28:10.131188Z"
    },
    "id": "XXcBIO-bpW4c"
   },
   "outputs": [],
   "source": [
    "# Step 1: Data Preparation\n",
    "\n",
    "def prepare_data(batch_size=16):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ])\n",
    "    dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    val_loader =0\n",
    "    train_loader = test_dataset\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T20:28:16.449448Z",
     "iopub.status.busy": "2024-07-01T20:28:16.449094Z",
     "iopub.status.idle": "2024-07-01T20:28:16.456135Z",
     "shell.execute_reply": "2024-07-01T20:28:16.455125Z",
     "shell.execute_reply.started": "2024-07-01T20:28:16.449418Z"
    },
    "id": "sUmzaSyC8y5h"
   },
   "outputs": [],
   "source": [
    "# Step 2: ResNet-18 Modification\n",
    "class ModifiedResNet18(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ModifiedResNet18, self).__init__()\n",
    "        self.resnet = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.DEFAULT)\n",
    "        self.resnet.fc = nn.Sequential(\n",
    "            nn.Linear(self.resnet.fc.in_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T20:28:25.465908Z",
     "iopub.status.busy": "2024-07-01T20:28:25.464685Z",
     "iopub.status.idle": "2024-07-01T20:28:25.475784Z",
     "shell.execute_reply": "2024-07-01T20:28:25.474880Z",
     "shell.execute_reply.started": "2024-07-01T20:28:25.465870Z"
    },
    "id": "JQ3uDk7HyA8P"
   },
   "outputs": [],
   "source": [
    "# Step 3: SISA Training Implementation\n",
    "\n",
    "def train_sisa_model(train_loader, val_loader, num_classes, S, R, epochs=1):\n",
    "    shard_size = len(train_loader) // S\n",
    "\n",
    "    models = []\n",
    "\n",
    "    for s in range(S):\n",
    "        print(s)\n",
    "        shard_data = torch.utils.data.Subset(train_loader, list(range(s * shard_size, (s + 1) * shard_size)))\n",
    "\n",
    "        # shard_loader = DataLoader(shard_data, batch_size=train_loader.batch_size, shuffle=True)\n",
    "        shard_loader= shard_data\n",
    "\n",
    "        model = ModifiedResNet18(num_classes).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        for r in range(R):\n",
    "            slice_size = shard_size // R\n",
    "            slice_data = torch.utils.data.Subset(shard_loader.dataset, list(range(r * slice_size, (r + 1) * slice_size)))\n",
    "            slice_loader = DataLoader(slice_data, batch_size=16, shuffle=True)\n",
    "\n",
    "\n",
    "            model.train()\n",
    "            for epoch in range(epochs):\n",
    "\n",
    "                for inputs, labels in slice_loader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            models.append(model)\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T20:28:29.053364Z",
     "iopub.status.busy": "2024-07-01T20:28:29.053017Z",
     "iopub.status.idle": "2024-07-01T20:28:29.062321Z",
     "shell.execute_reply": "2024-07-01T20:28:29.061345Z",
     "shell.execute_reply.started": "2024-07-01T20:28:29.053334Z"
    },
    "id": "yY8Xm9rM952m"
   },
   "outputs": [],
   "source": [
    "def aggregate_models(trained_model,dataloader, num_classes, device='cuda'):\n",
    "    all_probabilities = []\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for images, labels in dataloader:\n",
    "\n",
    "            images = images.to(device)\n",
    "            outputs = [model(images).cpu() for model in trained_model]\n",
    "            avg_output = sum(outputs) / len(outputs)\n",
    "            \n",
    "            probabilities = nn.Softmax(dim=1)(avg_output)\n",
    "            _, predictions = torch.max(probabilities, 1)\n",
    "            all_probabilities.extend(probabilities.numpy())\n",
    "            all_predictions.extend(predictions.numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "\n",
    "    return all_probabilities, all_predictions, all_labels\n",
    "\n",
    "\n",
    "def evaluate_model( probabilities, predictions,labels):\n",
    "\n",
    "    f1 = f1_score(labels, predictions,average='macro')\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision = precision_score( labels, predictions, average='macro')\n",
    "    recall = recall_score(labels,predictions, average='macro')\n",
    "\n",
    "    auroc = roc_auc_score(labels,probabilities, multi_class='ovr')\n",
    "    return f1, accuracy,precision, recall,auroc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our proposed aggregation methods are:\n",
    "\n",
    "1. Majority Voting: Each constituent model votes for a class label, and the majority label is\n",
    "output. Strength is simplicity, it effectively averages out errors. Weakness is that it only uses\n",
    "hard class labels rather than probabilities, and accurate constituents may be overwhelmed by\n",
    "many inaccurate ones.\n",
    "2. Averaging Probabilities: Each constituent outputs a probability distribution over classes, and\n",
    "the average distribution is output. Strength is it makes use of probability information rather\n",
    "than just labels. Weakness is that it assumes constituents are reasonably well-calibrated, and\n",
    "accuracy may decrease if some constituents are very inaccurate.\n",
    "3. Stacked Generalization: Train a meta-learner on the outputs of the constituent models. Strength\n",
    "is the meta-learner can learn the best way to combine the constituents based on their actual\n",
    "performance, mitigating weaknesses of simple averaging. Weakness is increased complexity,\n",
    "and may overfit if not regularized properly given the small number of constituent models.\n",
    "\n",
    "The paper found that for simpler learning tasks on datasets like Purchase and SVHN, both\n",
    "majority voting and averaging probabilities worked reasonably well with SISA training and did not\n",
    "significantly degrade accuracy compared to the baseline.\n",
    "\n",
    "Method number 2, \"Averaging Probabilities,\" can be considered better in this context.\n",
    "Unlike the \"Majority Voting\" method, which only considers hard class labels, \"Averaging Probabilities\" takes into account the probability distribution over classes generated by each constituent model. This allows for a more nuanced understanding of the confidence or uncertainty associated with each prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-07-01T20:28:58.562796Z",
     "iopub.status.busy": "2024-07-01T20:28:58.562416Z",
     "iopub.status.idle": "2024-07-01T20:29:00.138032Z",
     "shell.execute_reply": "2024-07-01T20:29:00.137236Z",
     "shell.execute_reply.started": "2024-07-01T20:28:58.562763Z"
    },
    "id": "6JD882Fo0RNJ",
    "outputId": "92468b61-2094-4d1f-fdf1-a662e997b984"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = prepare_data(batch_size=64)\n",
    "num_classes = 10  # CIFAR-10 has 10 classes\n",
    "\n",
    "S_values = [5, 10, 20]\n",
    "R_values = [5, 10, 20]\n",
    "\n",
    "# S_values = [5]\n",
    "# R_values = [5]\n",
    "\n",
    "results = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-06-29T19:37:19.981909Z",
     "iopub.status.busy": "2024-06-29T19:37:19.981516Z",
     "iopub.status.idle": "2024-06-29T19:42:52.156714Z",
     "shell.execute_reply": "2024-06-29T19:42:52.155910Z",
     "shell.execute_reply.started": "2024-06-29T19:37:19.981880Z"
    },
    "id": "-l4IxGfgpM2O",
    "outputId": "1e3101ef-f1d0-4d07-c40e-631c57a40245"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:00<00:00, 122MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "initial_model_states = {}\n",
    "\n",
    "# S=R=5\n",
    "for S in S_values:\n",
    "    for R in R_values:\n",
    "\n",
    "        modelss = train_sisa_model(train_loader, val_loader, num_classes, S, R, epochs=1)\n",
    "        initial_model_states[(S, R)] = modelss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-06-29T10:26:25.555582Z",
     "iopub.status.busy": "2024-06-29T10:26:25.555202Z",
     "iopub.status.idle": "2024-06-29T10:27:12.380744Z",
     "shell.execute_reply": "2024-06-29T10:27:12.379183Z",
     "shell.execute_reply.started": "2024-06-29T10:26:25.555550Z"
    },
    "id": "CqrWyoo4pM2P",
    "outputId": "189529d1-8ece-44c0-c6ab-f303978ab3f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "\n",
      "S: 5, R: 5, F1: 0.7378659002734873, Accuracy: 0.7391, Precision: 0.7414161543071336, Recall: 0.7390999999999999, AUROC: 0.9642954777777778\n",
      "\n",
      "S: 5, R: 10, F1: 0.6126626651784358, Accuracy: 0.6208, Precision: 0.6414155216951756, Recall: 0.6208, AUROC: 0.936843311111111\n",
      "\n",
      "S: 5, R: 20, F1: 0.7001602609672454, Accuracy: 0.699, Precision: 0.7110040071795748, Recall: 0.6990000000000001, AUROC: 0.9540885333333333\n",
      "\n",
      "10\n",
      "\n",
      "S: 10, R: 5, F1: 0.6070439250451045, Accuracy: 0.6107, Precision: 0.6859478630519296, Recall: 0.6106999999999999, AUROC: 0.9361826444444444\n",
      "\n",
      "S: 10, R: 10, F1: 0.6249221724759357, Accuracy: 0.6343, Precision: 0.6819560302081789, Recall: 0.6343, AUROC: 0.9378788777777778\n",
      "\n",
      "S: 10, R: 20, F1: 0.5301820971813231, Accuracy: 0.5512, Precision: 0.6400270334416847, Recall: 0.5511999999999999, AUROC: 0.9167874833333334\n",
      "\n",
      "20\n",
      "\n",
      "S: 20, R: 5, F1: 0.6596517389308058, Accuracy: 0.6579, Precision: 0.6701503678289822, Recall: 0.6579, AUROC: 0.943767688888889\n"
     ]
    }
   ],
   "source": [
    "for S in S_values:\n",
    "    print(S)\n",
    "    for R in R_values:\n",
    "        # Train initial models\n",
    "\n",
    "        modelss = initial_model_states[(S, R)]\n",
    "        probabilities, predictions, labels = aggregate_models(modelss, test_loader, num_classes)\n",
    "        f1, accuracy, precision, recall, auroc = evaluate_model(probabilities, predictions, labels)\n",
    "\n",
    "        print(f\"S: {S}, R: {R}, F1: {f1}, Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, AUROC: {auroc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T20:05:13.745855Z",
     "iopub.status.busy": "2024-06-29T20:05:13.745467Z",
     "iopub.status.idle": "2024-06-29T21:42:16.332209Z",
     "shell.execute_reply": "2024-06-29T21:42:16.331200Z",
     "shell.execute_reply.started": "2024-06-29T20:05:13.745829Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S: 20, R: 10, F1: 0.3516821968340481, Accuracy: 0.3766, Precision: 0.41011125168073564, Recall: 0.3766, AUROC: 0.8424659444444444\n",
      "S: 20, R: 20, F1: 0.2659517278318598, Accuracy: 0.3304, Precision: 0.48025230832906063, Recall: 0.3304, AUROC: 0.8548166888888888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "S=20\n",
    "R_values_2 = [10, 20]\n",
    "\n",
    "for R in R_values_2:\n",
    "        # Train initial models\n",
    "\n",
    "    modelss = initial_model_states[(S, R)]\n",
    "    probabilities, predictions, labels = aggregate_models(modelss, test_loader, num_classes)\n",
    "    f1, accuracy, precision, recall, auroc = evaluate_model(probabilities, predictions, labels)\n",
    "\n",
    "        # results.append((S, R, f1, accuracy, precision, recall, auroc))\n",
    "\n",
    "    print(f\"S: {S}, R: {R}, F1: {f1}, Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, AUROC: {auroc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best accurecy is resulted for S=R=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T20:29:14.871000Z",
     "iopub.status.busy": "2024-07-01T20:29:14.870088Z",
     "iopub.status.idle": "2024-07-01T20:35:56.453790Z",
     "shell.execute_reply": "2024-07-01T20:35:56.452793Z",
     "shell.execute_reply.started": "2024-07-01T20:29:14.870956Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:00<00:00, 81.4MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "S: 5, R: 5, F1: 0.8856942628312907, Accuracy: 0.8859, Precision: 0.8867759705110402, Recall: 0.8859, AUROC: 0.9916806444444445\n"
     ]
    }
   ],
   "source": [
    "S=5\n",
    "R=5\n",
    "\n",
    "modelss = train_sisa_model(train_loader, val_loader, num_classes, S, R, epochs=5)\n",
    "probabilities, predictions, labels = aggregate_models(modelss, test_loader, num_classes)\n",
    "f1, accuracy, precision, recall, auroc = evaluate_model(probabilities, predictions, labels)\n",
    "\n",
    "print(f\"S: {S}, R: {R}, F1: {f1}, Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, AUROC: {auroc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dUhuBlaCR8iU"
   },
   "source": [
    "$Simulation Question 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T04:51:42.748657Z",
     "iopub.status.busy": "2024-06-30T04:51:42.748059Z",
     "iopub.status.idle": "2024-06-30T04:51:42.753432Z",
     "shell.execute_reply": "2024-06-30T04:51:42.752475Z",
     "shell.execute_reply.started": "2024-06-30T04:51:42.748624Z"
    },
    "id": "is8HKEu-R_W4"
   },
   "outputs": [],
   "source": [
    "# Step 1: Identify and remove data points to be unlearned\n",
    "def identify_data_to_unlearn(dataset, num_to_unlearn=500):\n",
    "    all_indices = list(range(len(dataset)))\n",
    "    indices_to_unlearn = random.sample(all_indices, num_to_unlearn)\n",
    "    return indices_to_unlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T04:51:44.723645Z",
     "iopub.status.busy": "2024-06-30T04:51:44.722859Z",
     "iopub.status.idle": "2024-06-30T04:51:44.734425Z",
     "shell.execute_reply": "2024-06-30T04:51:44.733460Z",
     "shell.execute_reply.started": "2024-06-30T04:51:44.723611Z"
    },
    "id": "qFCztQHHSB9k"
   },
   "outputs": [],
   "source": [
    "# Step 2: Unlearn data from the relevant shards and slices\n",
    "def unlearn_data(models, train_loader, num_classes, S, R, indices_to_unlearn, epochs=1):\n",
    "    shard_size = len(train_loader) // S\n",
    "    updated_models = []\n",
    "    for s in range(S):\n",
    "        shard_start = s * shard_size\n",
    "        shard_end = (s + 1) * shard_size\n",
    "        shard_indices = set(range(shard_start, shard_end))\n",
    "        unlearn_indices_in_shard = shard_indices.intersection(indices_to_unlearn)\n",
    "\n",
    "        if unlearn_indices_in_shard:\n",
    "            shard_data = torch.utils.data.Subset(train_loader, list(shard_indices - unlearn_indices_in_shard))\n",
    "#             shard_loader = DataLoader(shard_data, batch_size=64, shuffle=True)\n",
    "            shard_loader = shard_data\n",
    "            model = ModifiedResNet18(num_classes).cuda()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "            for r in range(R):\n",
    "                slice_size = shard_size // R\n",
    "                slice_data = torch.utils.data.Subset(shard_loader.dataset, list(range(r * slice_size, (r + 1) * slice_size)))\n",
    "                slice_loader = DataLoader(slice_data, batch_size=16, shuffle=True)\n",
    "\n",
    "                for epoch in range(epochs):\n",
    "                    model.train()\n",
    "                    for inputs, labels in slice_loader:\n",
    "                        inputs, labels = inputs.to(device), labels.to(device)\n",
    "                        optimizer.zero_grad()\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                updated_models.append(model)\n",
    "        else:\n",
    "            updated_models.append(models[s])\n",
    "\n",
    "    return updated_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code calculates the size of each shard based on the length of the training loader and the number of shards.\n",
    "The code then iterates over each shard (s) from 0 to S-1.\n",
    "For each shard, it identifies the start and end indices of the shard in the training data.\n",
    "It checks if any of the indices to unlearn fall within the current shard. If there are unlearn indices, the code proceeds to unlearn the data in that shard.\n",
    "If there are unlearn indices in the shard, a new subset of the training data is created by excluding the unlearn indices using torch.utils.data.Subset.\n",
    "\n",
    "The code then enters a loop that iterates R times, representing the slices within the shard.\n",
    "For each slice, a new subset of the shard data is created.\n",
    "\n",
    "The code enters another loop that runs for epochs times.\n",
    "Within each epoch, the model is put in training mode, and for each batch of inputs and labels in the slice loader, the optimization steps are performed: zeroing the gradients, computing the outputs, calculating the loss, backpropagating the gradients, and updating the model parameters.\n",
    "\n",
    "After the training loops, the updated model is appended to the updated_models list.\n",
    "If there are no unlearn indices in the shard, the original model from the models list is appended to the updated_models list without any modification.\n",
    "\n",
    "Once all shards have been processed, the function returns the updated_models list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T04:51:48.178508Z",
     "iopub.status.busy": "2024-06-30T04:51:48.177815Z",
     "iopub.status.idle": "2024-06-30T04:51:48.183836Z",
     "shell.execute_reply": "2024-06-30T04:51:48.182851Z",
     "shell.execute_reply.started": "2024-06-30T04:51:48.178476Z"
    },
    "id": "h1SqUN7fSEPR"
   },
   "outputs": [],
   "source": [
    "# Step 3: Evaluate performance metrics\n",
    "def evaluate_unlearning_performance(test_loader, models, num_classes):\n",
    "    start_time = time.time()\n",
    "\n",
    "    probabilities, predictions, labels = aggregate_models(models, test_loader,num_classes)\n",
    "    f1, accuracy, precision, recall, auroc = evaluate_model(probabilities, predictions, labels)\n",
    "\n",
    "    unlearning_time = time.time() - start_time\n",
    "    return f1, accuracy, precision, recall, auroc, unlearning_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T19:45:38.956826Z",
     "iopub.status.busy": "2024-06-29T19:45:38.956286Z",
     "iopub.status.idle": "2024-06-29T19:45:38.974869Z",
     "shell.execute_reply": "2024-06-29T19:45:38.973810Z",
     "shell.execute_reply.started": "2024-06-29T19:45:38.956789Z"
    }
   },
   "outputs": [],
   "source": [
    "initial_results = []\n",
    "unlearning_results = []\n",
    "num_to_unlearn = 500\n",
    "\n",
    "indices_to_unlearn_dict = {}\n",
    "\n",
    "for S in S_values:\n",
    "    for R in R_values:\n",
    "\n",
    "            # Train initial models\n",
    "            \n",
    "        initial_models = initial_model_states[(S, R)]\n",
    "\n",
    "            # Identify data points to be unlearned\n",
    "        indices_to_unlearn = identify_data_to_unlearn(train_loader, num_to_unlearn=num_to_unlearn)\n",
    "        indices_to_unlearn_dict[(S, R)] = indices_to_unlearn\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-06-29T14:21:11.823660Z",
     "iopub.status.busy": "2024-06-29T14:21:11.822592Z",
     "iopub.status.idle": "2024-06-29T17:45:33.490822Z",
     "shell.execute_reply": "2024-06-29T17:45:33.489830Z",
     "shell.execute_reply.started": "2024-06-29T14:21:11.823613Z"
    },
    "id": "8ptLsTyVSFvE",
    "outputId": "7207083e-f835-4188-9954-cddc071091ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unlearn - S: 5, R: 5, F1: 0.7633936614861464, Accuracy: 0.7629, Precision: 0.7664485311405249, Recall: 0.7629, AUROC: 0.9700106777777778, Time: 244.53131890296936\n",
      "Unlearn - S: 5, R: 10, F1: 0.6530344699052529, Accuracy: 0.6566, Precision: 0.6669224856746666, Recall: 0.6566, AUROC: 0.9434641277777777, Time: 474.72060441970825\n",
      "Unlearn - S: 5, R: 20, F1: 0.6705007734050479, Accuracy: 0.6715, Precision: 0.6858269915247638, Recall: 0.6715, AUROC: 0.9458656555555557, Time: 932.6459593772888\n",
      "Unlearn - S: 10, R: 5, F1: 0.5879710195672135, Accuracy: 0.5956, Precision: 0.6626835431671212, Recall: 0.5955999999999999, AUROC: 0.9293068, Time: 473.94844698905945\n",
      "Unlearn - S: 10, R: 10, F1: 0.6125650379884067, Accuracy: 0.6216, Precision: 0.6323818961206391, Recall: 0.6216, AUROC: 0.9300428777777776, Time: 933.6839303970337\n",
      "Unlearn - S: 10, R: 20, F1: 0.499533264202629, Accuracy: 0.5249, Precision: 0.6055703711905982, Recall: 0.5249, AUROC: 0.9015256111111111, Time: 1848.2416596412659\n",
      "Unlearn - S: 20, R: 5, F1: 0.6180572858889791, Accuracy: 0.6173, Precision: 0.6457657538839029, Recall: 0.6173, AUROC: 0.9303175777777778, Time: 932.2650172710419\n",
      "Unlearn - S: 20, R: 10, F1: 0.5824918803923859, Accuracy: 0.5828, Precision: 0.607805681310947, Recall: 0.5828, AUROC: 0.9185197888888886, Time: 1853.833814382553\n",
      "Unlearn - S: 20, R: 20, F1: 0.38886592089939676, Accuracy: 0.3995, Precision: 0.4675344890988763, Recall: 0.3995, AUROC: 0.8567489944444444, Time: 3693.6543641090393\n"
     ]
    }
   ],
   "source": [
    "# Main Function\n",
    "\n",
    "for S in S_values:\n",
    "    for R in R_values:\n",
    "\n",
    "        indices_to_unlearn = indices_to_unlearn_dict[(S, R)]\n",
    "\n",
    "        # Unlearn data\n",
    "        updated_models = unlearn_data(initial_models, train_loader, num_classes, S, R, indices_to_unlearn, epochs=3)\n",
    "\n",
    "        # Evaluate unlearning performance\n",
    "        unlearn_f1, unlearn_accuracy, unlearn_precision, unlearn_recall, unlearn_auroc, unlearning_time = evaluate_unlearning_performance(test_loader, updated_models, num_classes)\n",
    "        unlearning_results.append((S, R, unlearn_f1, unlearn_accuracy, unlearn_precision, unlearn_recall, unlearn_auroc, unlearning_time))\n",
    "\n",
    "        #print(f\"Initial - S: {S}, R: {R}, F1: {initial_f1}, Accuracy: {initial_accuracy}, Precision: {initial_precision}, Recall: {initial_recall}, AUROC: {initial_auroc}\")\n",
    "        print(f\"Unlearn - S: {S}, R: {R}, F1: {unlearn_f1}, Accuracy: {unlearn_accuracy}, Precision: {unlearn_precision}, Recall: {unlearn_recall}, AUROC: {unlearn_auroc}, Time: {unlearning_time}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHedcTvYA-0k"
   },
   "source": [
    "$Simulation Question 3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T04:51:52.783557Z",
     "iopub.status.busy": "2024-06-30T04:51:52.782682Z",
     "iopub.status.idle": "2024-06-30T04:51:52.789159Z",
     "shell.execute_reply": "2024-06-30T04:51:52.788074Z",
     "shell.execute_reply.started": "2024-06-30T04:51:52.783522Z"
    },
    "id": "tL5mlXhhDC8l"
   },
   "outputs": [],
   "source": [
    "def compute_losses(model, data_loader, device='cuda'):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            outputs = model(inputs)\n",
    "            loss = F.cross_entropy(outputs, labels, reduction='none')\n",
    "            losses.extend(loss.cpu().numpy())\n",
    "            \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T04:51:54.903029Z",
     "iopub.status.busy": "2024-06-30T04:51:54.902672Z",
     "iopub.status.idle": "2024-06-30T04:51:54.909187Z",
     "shell.execute_reply": "2024-06-30T04:51:54.908290Z",
     "shell.execute_reply.started": "2024-06-30T04:51:54.902999Z"
    },
    "id": "NceEcHBreNQm"
   },
   "outputs": [],
   "source": [
    "def membership_inference_attack(losses_forget, losses_test):\n",
    "\n",
    "    X = np.concatenate([losses_forget, losses_test]).reshape(-1, 1)\n",
    "    y = np.concatenate([np.ones(len(losses_forget)), np.zeros(len(losses_test))])\n",
    "    min_samples = min(len(losses_forget), len(losses_test))\n",
    "    cv_splits = max(2, min(3, min_samples))\n",
    "    clf = LogisticRegressionCV(cv=cv_splits).fit(X, y)\n",
    "    scores = cross_val_score(clf, X, y, cv=cv_splits)\n",
    "\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code concatenates the losses from both sets into a single array X.\n",
    "It creates a target array y where it assigns a label of 1 to the losses from losses_forget and a label of 0 to the losses from losses_test.\n",
    "A logistic regression classifier with cross-validation (LogisticRegressionCV) is fitted to the data (X and y).\n",
    "\n",
    "Cross-validation scores are computed using cross_val_score to evaluate the performance of the classifier.\n",
    "\n",
    "The mean of the cross-validation scores is returned as the attack score.\n",
    "\n",
    "The membership_inference_attack function combines the losses from both the training and test data, trains a logistic regression classifier using cross-validation, and returns the mean cross-validation score as the attack score. \n",
    "\n",
    "This score indicates the success of the membership inference attack in distinguishing between data points used for training and those not used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T04:51:58.078803Z",
     "iopub.status.busy": "2024-06-30T04:51:58.077900Z",
     "iopub.status.idle": "2024-06-30T04:51:58.085664Z",
     "shell.execute_reply": "2024-06-30T04:51:58.084594Z",
     "shell.execute_reply.started": "2024-06-30T04:51:58.078768Z"
    },
    "id": "uywLHFjSRwxW"
   },
   "outputs": [],
   "source": [
    "def evaluate_unlearning_performance_3(forget_set_loader, test_loader, initial_model, unlearned_model, num_test_samples):\n",
    "\n",
    "    initial_losses_forget = compute_losses(initial_model, forget_set_loader)\n",
    "    initial_losses_test = compute_losses(initial_model, test_loader)\n",
    "\n",
    "    test_indices = list(range(len(initial_losses_test)))\n",
    "    random_test_indices = random.sample(test_indices, num_test_samples)\n",
    "    sampled_test_losses_initial = [initial_losses_test[i] for i in random_test_indices]\n",
    "\n",
    "    # Perform MIA on initial model\n",
    "    initial_mia_score = membership_inference_attack(initial_losses_forget, sampled_test_losses_initial)\n",
    "\n",
    "    unlearned_losses_forget = compute_losses(unlearned_model, forget_set_loader)\n",
    "    unlearned_losses_test = compute_losses(unlearned_model, test_loader)\n",
    "    sampled_test_losses_unlearned = [unlearned_losses_test[i] for i in random_test_indices]\n",
    "\n",
    "    # Perform MIA on unlearned model\n",
    "    unlearned_mia_score = membership_inference_attack(unlearned_losses_forget, sampled_test_losses_unlearned)\n",
    "\n",
    "    return initial_mia_score, unlearned_mia_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2024-06-29T19:03:36.338639Z",
     "iopub.status.busy": "2024-06-29T19:03:36.337795Z",
     "iopub.status.idle": "2024-06-29T19:13:29.087905Z",
     "shell.execute_reply": "2024-06-29T19:13:29.086709Z",
     "shell.execute_reply.started": "2024-06-29T19:03:36.338609Z"
    },
    "id": "5QfwCjDIDHOp",
    "outputId": "a8351200-f75d-4dca-ea92-e36bc6ab87c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "S: 5, R: 5\n",
      "Initial MIA Score: 0.490990990990991\n",
      "Unlearned MIA Score: 0.5100279920639202\n",
      "S: 5, R: 10\n",
      "Initial MIA Score: 0.46601092110074144\n",
      "Unlearned MIA Score: 0.49899899899899897\n",
      "S: 5, R: 20\n",
      "Initial MIA Score: 0.47002091912271554\n",
      "Unlearned MIA Score: 0.4860249471027915\n",
      "S: 10, R: 5\n",
      "Initial MIA Score: 0.49101496706287123\n",
      "Unlearned MIA Score: 0.499997002991015\n",
      "S: 10, R: 10\n",
      "Initial MIA Score: 0.4920129710548872\n",
      "Unlearned MIA Score: 0.48799398200595806\n",
      "S: 10, R: 20\n",
      "Initial MIA Score: 0.48297399195602786\n",
      "Unlearned MIA Score: 0.5159320997644351\n",
      "S: 20, R: 5\n",
      "Initial MIA Score: 0.506006006006006\n",
      "Unlearned MIA Score: 0.4999760239281197\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 91.06 MiB is free. Process 2725 has 14.66 GiB memory in use. Of the allocated memory 13.78 GiB is allocated by PyTorch, and 740.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m             forget_set_loader \u001b[38;5;241m=\u001b[39m DataLoader(forget_set_data, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     28\u001b[0m                             \u001b[38;5;66;03m# Unlearn data\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m             updated_model_states \u001b[38;5;241m=\u001b[39m \u001b[43munlearn_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_model_states_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices_to_unlearn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m             unlearned_model \u001b[38;5;241m=\u001b[39m ModifiedResNet18(num_classes)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#             for state in updated_model_states:\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#                     unlearned_model.load_state_dict(state)\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \n\u001b[1;32m     34\u001b[0m                             \u001b[38;5;66;03m# Evaluate unlearning performance\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[50], line 31\u001b[0m, in \u001b[0;36munlearn_data\u001b[0;34m(models, train_loader, num_classes, S, R, indices_to_unlearn, epochs)\u001b[0m\n\u001b[1;32m     29\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m     30\u001b[0m         loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> 31\u001b[0m         \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     34\u001b[0m updated_models\u001b[38;5;241m.\u001b[39mappend(model)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 91.06 MiB is free. Process 2725 has 14.66 GiB memory in use. Of the allocated memory 13.78 GiB is allocated by PyTorch, and 740.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_loader, val_loader, test_loader = prepare_data(batch_size=16)\n",
    "    num_classes = 10  # CIFAR-10 has 10 classes\n",
    "\n",
    "    S_values = [5, 10, 20]\n",
    "    R_values = [5, 10, 20]\n",
    "\n",
    "    initial_results = []\n",
    "    unlearning_results = []\n",
    "    num_to_unlearn = 500\n",
    "    num_test_samples= 500\n",
    "\n",
    "    for S in S_values:\n",
    "        for R in R_values:\n",
    "\n",
    "            # Train initial models\n",
    "            initial_model_states_1 = initial_model_states[(S, R)]\n",
    "            initial_model = ModifiedResNet18(num_classes).cuda()\n",
    "\n",
    "            indices_to_unlearn = indices_to_unlearn_dict[(S, R)]\n",
    "\n",
    "            # Prepare forget set loader\n",
    "            forget_set_data = torch.utils.data.Subset(train_loader, indices_to_unlearn)\n",
    "            forget_set_loader = DataLoader(forget_set_data, batch_size=64, shuffle=False)\n",
    "\n",
    "            # Unlearn data\n",
    "            updated_model_states = unlearn_data(initial_model_states_1, train_loader, num_classes, S, R, indices_to_unlearn, epochs=1)\n",
    "            unlearned_model = ModifiedResNet18(num_classes).cuda()\n",
    "\n",
    "            # Evaluate unlearning performance\n",
    "            initial_mia_score, unlearned_mia_score = evaluate_unlearning_performance_3(forget_set_loader, test_loader, initial_model, unlearned_model, num_test_samples)\n",
    "\n",
    "            print(f\"S: {S}, R: {R}\")\n",
    "            print(f\"Initial MIA Score: {initial_mia_score}\")\n",
    "            print(f\"Unlearned MIA Score: {unlearned_mia_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see CUDA is out of memory but I proceed to run the code from the first for the remaing S and R:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T19:47:38.588987Z",
     "iopub.status.busy": "2024-06-29T19:47:38.588635Z",
     "iopub.status.idle": "2024-06-29T19:50:31.513907Z",
     "shell.execute_reply": "2024-06-29T19:50:31.512935Z",
     "shell.execute_reply.started": "2024-06-29T19:47:38.588961Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S: 20, R: 10\n",
      "Initial MIA Score: 0.5069980159800519\n",
      "Unlearned MIA Score: 0.4879819939700179\n",
      "S: 20, R: 20\n",
      "Initial MIA Score: 0.4840289391187594\n",
      "Unlearned MIA Score: 0.48799398200595806\n"
     ]
    }
   ],
   "source": [
    "num_to_unlearn = 500\n",
    "num_test_samples= 500\n",
    "    \n",
    "S=20\n",
    "R_values_2 = [10, 20]\n",
    "    \n",
    "for R in R_values_2:\n",
    "\n",
    "    # Train initial models\n",
    "    initial_model_states_1 = initial_model_states[(S, R)]\n",
    "    initial_model = ModifiedResNet18(num_classes).cuda()\n",
    "\n",
    "    indices_to_unlearn = indices_to_unlearn_dict[(S, R)]\n",
    "\n",
    "    # Prepare forget set loader\n",
    "    forget_set_data = torch.utils.data.Subset(train_loader, indices_to_unlearn)\n",
    "    forget_set_loader = DataLoader(forget_set_data, batch_size=16, shuffle=False)\n",
    "\n",
    "    # Unlearn data\n",
    "    updated_model_states = unlearn_data(initial_model_states_1, train_loader, num_classes, S, R, indices_to_unlearn, epochs=1)\n",
    "    unlearned_model = ModifiedResNet18(num_classes).cuda()\n",
    "    \n",
    "    \n",
    "    # Evaluate unlearning performance\n",
    "    initial_mia_score, unlearned_mia_score = evaluate_unlearning_performance_3(forget_set_loader, test_loader, initial_model, unlearned_model, num_test_samples)\n",
    "\n",
    "    print(f\"S: {S}, R: {R}\")\n",
    "    print(f\"Initial MIA Score: {initial_mia_score}\")\n",
    "    print(f\"Unlearned MIA Score: {unlearned_mia_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bj7ekRoVpM2R"
   },
   "source": [
    "$Add On. Simulation Question 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a perfectly unlearned model, we would expect the following:\n",
    "\n",
    "Reduced Membership Inference Attack (MIA) Score: The cross-validation score of the Logistic Regression-based Membership Inference Attack should be significantly lower after unlearning. This would indicate that the model can no longer effectively distinguish between the data that was part of the training set (forget set) and the data that was not (randomly chosen test data). In an ideal scenario, the MIA score should be close to that of a random guess (around 0.5 for a binary classification task in a balanced dataset).\n",
    "\n",
    "Restored General Performance: The general performance metrics (F1-score, accuracy, precision, recall, and AUROC) on clean test data should not be significantly degraded after the unlearning process. This would demonstrate that the model has retained its ability to generalize well on clean data even after the unlearning.\n",
    "\n",
    "Reduced Attack Success Rate (ASR): For the backdoor attack, the ASR should be significantly lower after unlearning. This indicates that the model has successfully \"forgotten\" the association between the backdoor trigger and the target label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S: 20, R: 5\n",
    "Initial MIA Score: 0.506006006006006\n",
    "Unlearned MIA Score: 0.4999760239281197"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T04:52:07.692445Z",
     "iopub.status.busy": "2024-06-30T04:52:07.691809Z",
     "iopub.status.idle": "2024-06-30T04:52:07.699772Z",
     "shell.execute_reply": "2024-06-30T04:52:07.698607Z",
     "shell.execute_reply.started": "2024-06-30T04:52:07.692411Z"
    },
    "id": "IkWeyGtypM2R"
   },
   "outputs": [],
   "source": [
    "def poison_data(dataset, num_samples=500, target_class=0):\n",
    "    poisoned_indices = []\n",
    "    poisoned_data = []\n",
    "\n",
    "    # Select indices of the target class\n",
    "    target_indices = [i for i, (_, label) in enumerate(dataset) if label == target_class]\n",
    "    selected_indices = random.sample(target_indices, num_samples)\n",
    "\n",
    "    for idx in selected_indices:\n",
    "        img, label = dataset[idx]\n",
    "        img = np.array(img)\n",
    "        \n",
    "        # Randomly choose the position of the 3x3 black block\n",
    "        x = random.randint(0, img.shape[1] - 3)\n",
    "        y = random.randint(0, img.shape[2] - 3)\n",
    "        img[:, x:x+3, y:y+3] = 0  # Set the block to black\n",
    "        poisoned_data.append((torch.tensor(img), label))\n",
    "        poisoned_indices.append(idx)\n",
    "\n",
    "    return poisoned_data, poisoned_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T04:52:10.318329Z",
     "iopub.status.busy": "2024-06-30T04:52:10.317964Z",
     "iopub.status.idle": "2024-06-30T04:52:10.322948Z",
     "shell.execute_reply": "2024-06-30T04:52:10.322072Z",
     "shell.execute_reply.started": "2024-06-30T04:52:10.318299Z"
    },
    "id": "wVBjWhWWpM2R"
   },
   "outputs": [],
   "source": [
    "# Function to add poisoned data to the dataset\n",
    "def add_poisoned_data(train_loader, poisoned_data):\n",
    "    \n",
    "    poisoned_dataset = list(train_loader) + poisoned_data\n",
    "    poisoned_loader = poisoned_dataset\n",
    "    return poisoned_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T04:53:37.643569Z",
     "iopub.status.busy": "2024-06-30T04:53:37.643191Z",
     "iopub.status.idle": "2024-06-30T04:53:39.226816Z",
     "shell.execute_reply": "2024-06-30T04:53:39.226048Z",
     "shell.execute_reply.started": "2024-06-30T04:53:37.643540Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = prepare_data(batch_size=16)\n",
    "num_classes = 10  # CIFAR-10 has 10 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T04:53:41.498859Z",
     "iopub.status.busy": "2024-06-30T04:53:41.498512Z",
     "iopub.status.idle": "2024-06-30T04:54:07.494436Z",
     "shell.execute_reply": "2024-06-30T04:54:07.493420Z",
     "shell.execute_reply.started": "2024-06-30T04:53:41.498825Z"
    },
    "id": "nnbiV_ozpM2S"
   },
   "outputs": [],
   "source": [
    "poisoned_data, poisoned_indices = poison_data(train_loader, num_samples=500, target_class=0)\n",
    "poisoned_train_loader = add_poisoned_data(train_loader, poisoned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T04:54:37.238023Z",
     "iopub.status.busy": "2024-06-30T04:54:37.237320Z",
     "iopub.status.idle": "2024-06-30T04:54:37.247847Z",
     "shell.execute_reply": "2024-06-30T04:54:37.246931Z",
     "shell.execute_reply.started": "2024-06-30T04:54:37.237992Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_sisa_model_11(train_loader, val_loader, num_classes, S, R, epochs=1):\n",
    "    shard_size = len(train_loader) // S\n",
    "\n",
    "    models = []\n",
    "\n",
    "    for s in range(S):\n",
    "        print(s)\n",
    "        shard_data = torch.utils.data.Subset(train_loader, list(range(s * shard_size, (s + 1) * shard_size)))\n",
    "\n",
    "        shard_loader= shard_data\n",
    "\n",
    "        model = ModifiedResNet18(num_classes).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        for r in range(R):\n",
    "            slice_size = shard_size // R\n",
    "            slice_data = torch.utils.data.Subset(shard_loader.dataset, list(range(r * slice_size, (r + 1) * slice_size)))\n",
    "            slice_loader = DataLoader(slice_data, batch_size=16, shuffle=True)\n",
    "\n",
    "\n",
    "            model.train()\n",
    "            for epoch in range(epochs):\n",
    "\n",
    "                for inputs, labels in slice_loader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            models.append(model)\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T04:54:40.584125Z",
     "iopub.status.busy": "2024-06-30T04:54:40.583206Z",
     "iopub.status.idle": "2024-06-30T04:55:20.505797Z",
     "shell.execute_reply": "2024-06-30T04:55:20.504805Z",
     "shell.execute_reply.started": "2024-06-30T04:54:40.584096Z"
    },
    "id": "t08aOsPApM2S",
    "outputId": "7c9d6084-489d-4a53-a6f8-1202a3c0201c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:00<00:00, 143MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "S = 20\n",
    "R = 5\n",
    "\n",
    "best_model_states = train_sisa_model_11(poisoned_train_loader, val_loader, num_classes, S, R, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T04:55:39.844271Z",
     "iopub.status.busy": "2024-06-30T04:55:39.843506Z",
     "iopub.status.idle": "2024-06-30T05:12:20.134235Z",
     "shell.execute_reply": "2024-06-30T05:12:20.133335Z",
     "shell.execute_reply.started": "2024-06-30T04:55:39.844216Z"
    },
    "id": "KJ6fZebEpM2S",
    "outputId": "4de836a7-bafe-4e6b-e498-dd4cec11749c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Test Data - F1: 0.40790780553316297, Accuracy: 0.4373, Precision: 0.46660492814549215, Recall: 0.4373, AUROC: 0.8657756055555555\n"
     ]
    }
   ],
   "source": [
    "probabilities_p, predictions_p, labels_p = aggregate_models(best_model_states, test_loader, num_classes)\n",
    "f1_p, accuracy_p, precision_p, recall_p, auroc_p = evaluate_model(probabilities_p, predictions_p, labels_p)\n",
    "\n",
    "print(f\"Clean Test Data - F1: {f1_p}, Accuracy: {accuracy_p}, Precision: {precision_p}, Recall: {recall_p}, AUROC: {auroc_p}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T05:16:07.625041Z",
     "iopub.status.busy": "2024-06-30T05:16:07.624443Z",
     "iopub.status.idle": "2024-06-30T05:16:07.631479Z",
     "shell.execute_reply": "2024-06-30T05:16:07.630489Z",
     "shell.execute_reply.started": "2024-06-30T05:16:07.625006Z"
    },
    "id": "D0hRkmW_pM2S"
   },
   "outputs": [],
   "source": [
    "def poison_test_data(test_loader, target_class=0):\n",
    "    poisoned_test_data = []\n",
    "\n",
    "    for img, label in test_loader.dataset:\n",
    "        img = np.array(img)\n",
    "        # Randomly choose the position of the 3x3 black block\n",
    "        x = random.randint(0, img.shape[1] - 3)\n",
    "        y = random.randint(0, img.shape[2] - 3)\n",
    "        img[:, x:x+3, y:y+3] = 0  # Set the block to black\n",
    "        poisoned_test_data.append((torch.tensor(img), target_class))\n",
    "\n",
    "    return poisoned_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T05:16:11.805211Z",
     "iopub.status.busy": "2024-06-30T05:16:11.804385Z",
     "iopub.status.idle": "2024-06-30T05:33:00.703688Z",
     "shell.execute_reply": "2024-06-30T05:33:00.702661Z",
     "shell.execute_reply.started": "2024-06-30T05:16:11.805180Z"
    },
    "id": "JsP1tDStpM2T",
    "outputId": "e142c140-8338-46c3-f141-3165d781ec17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack Success Rate (ASR): 0.11\n"
     ]
    }
   ],
   "source": [
    "poisoned_test_data = poison_test_data(test_loader, target_class=0)\n",
    "poisoned_test_loader = DataLoader(poisoned_test_data, batch_size=test_loader.batch_size, shuffle=False)\n",
    "\n",
    "# Evaluate on poisoned test data\n",
    "probabilities_pp, predictions_pp, labels_pp = aggregate_models(best_model_states, poisoned_test_loader, num_classes)\n",
    "asr = (np.array(predictions_pp) == 0).mean()  # ASR is the percentage of samples misclassified as the target class\n",
    "\n",
    "print(f\"Attack Success Rate (ASR): {asr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is a good score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jrTTEWmpM2T"
   },
   "source": [
    "$Add On. Simulation Question 2.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T05:41:36.180079Z",
     "iopub.status.busy": "2024-06-30T05:41:36.179143Z",
     "iopub.status.idle": "2024-06-30T05:42:15.884345Z",
     "shell.execute_reply": "2024-06-30T05:42:15.883485Z",
     "shell.execute_reply.started": "2024-06-30T05:41:36.180031Z"
    },
    "id": "N_RxdfacpM2T"
   },
   "outputs": [],
   "source": [
    "indices_to_unlearn = poisoned_indices\n",
    "unlearned_model_states = unlearn_data(best_model_states, train_loader, num_classes, S, R, indices_to_unlearn, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T05:42:21.449824Z",
     "iopub.status.busy": "2024-06-30T05:42:21.449459Z",
     "iopub.status.idle": "2024-06-30T05:59:06.908138Z",
     "shell.execute_reply": "2024-06-30T05:59:06.907006Z",
     "shell.execute_reply.started": "2024-06-30T05:42:21.449796Z"
    },
    "id": "aMzMhIjipM2T",
    "outputId": "dbb8939d-e93a-46f0-f9fd-ead9af1a85ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Test Data After Unlearning - F1: 0.39386923133051766, Accuracy: 0.4254, Precision: 0.4314379862844909, Recall: 0.42539999999999994, AUROC: 0.8539407777777777\n"
     ]
    }
   ],
   "source": [
    "probabilities, predictions, labels = aggregate_models(unlearned_model_states, test_loader, num_classes)\n",
    "f1, accuracy, precision, recall, auroc = evaluate_model(probabilities, predictions, labels)\n",
    "\n",
    "print(f\"Clean Test Data After Unlearning - F1: {f1}, Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, AUROC: {auroc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T06:00:20.159754Z",
     "iopub.status.busy": "2024-06-30T06:00:20.159073Z",
     "iopub.status.idle": "2024-06-30T06:16:56.319442Z",
     "shell.execute_reply": "2024-06-30T06:16:56.318492Z",
     "shell.execute_reply.started": "2024-06-30T06:00:20.159720Z"
    },
    "id": "NgyRnIQzpM2T",
    "outputId": "0faec533-ffba-4840-fd03-ba207f77c184"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack Success Rate (ASR) After Unlearning: 0.0553\n"
     ]
    }
   ],
   "source": [
    "probabilities, predictions, labels = aggregate_models(unlearned_model_states, poisoned_test_loader, num_classes)\n",
    "asr = (np.array(predictions) == 0).mean()\n",
    "print(f\"Attack Success Rate (ASR) After Unlearning: {asr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see after the unlerning the score is reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
