{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0b29fee416ec4603b352996738065f3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_10c3f1c5621a4a8eac3f096a3bace128",
              "IPY_MODEL_59f9e510c3094e049033a83d19d5d748",
              "IPY_MODEL_3d63a47c46c14a8b9dfc7ebff5db9498"
            ],
            "layout": "IPY_MODEL_208c7b82077c45d3aaf0d686c7e7e512"
          }
        },
        "10c3f1c5621a4a8eac3f096a3bace128": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d50bcdd7eec449d38fa7666afcbfabe4",
            "placeholder": "​",
            "style": "IPY_MODEL_9ce287b19f794d13820e4301b645ad0d",
            "value": "Epoch: 100%"
          }
        },
        "59f9e510c3094e049033a83d19d5d748": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_824abfbb73b74df0954a6fcc7f481bcf",
            "max": 15,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_513b63052f5942b5a0cce75b7954b4d0",
            "value": 15
          }
        },
        "3d63a47c46c14a8b9dfc7ebff5db9498": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99664359c2324868ba2c3fb22cbbd4c9",
            "placeholder": "​",
            "style": "IPY_MODEL_da2bfc12d1af469db0185e293a906a52",
            "value": " 15/15 [05:59&lt;00:00, 27.85s/epoch]"
          }
        },
        "208c7b82077c45d3aaf0d686c7e7e512": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d50bcdd7eec449d38fa7666afcbfabe4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ce287b19f794d13820e4301b645ad0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "824abfbb73b74df0954a6fcc7f481bcf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "513b63052f5942b5a0cce75b7954b4d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "99664359c2324868ba2c3fb22cbbd4c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da2bfc12d1af469db0185e293a906a52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# IML - Project\n",
        "## Hossein Anjidani & Zahra Maleki\n"
      ],
      "metadata": {
        "id": "muAf63iqW1sg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_Mu0FBiWrR6",
        "outputId": "26641b20-ea2b-4d2e-a7b6-38f2227a520b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hossein Anjidani 400100746\n",
            "Zahra Maleki 400110009\n"
          ]
        }
      ],
      "source": [
        "name1 = \"Hossein Anjidani\"\n",
        "name2 = \"Zahra Maleki\"\n",
        "st1 = 400100746\n",
        "st2 = 400110009\n",
        "print(name1, st1)\n",
        "print(name2, st2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Private Training - Question 4"
      ],
      "metadata": {
        "id": "wcbJiXGhYrl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install necessary components\n",
        "!pip install opacus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-T7eo3OYCPo",
        "outputId": "b0fa842f-7be8-414d-af6a-6a885dd638c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opacus in /usr/local/lib/python3.10/dist-packages (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.10/dist-packages (from opacus) (1.25.2)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.10/dist-packages (from opacus) (2.3.0+cu121)\n",
            "Requirement already satisfied: scipy>=1.2 in /usr/local/lib/python3.10/dist-packages (from opacus) (1.11.4)\n",
            "Requirement already satisfied: opt-einsum>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from opacus) (3.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0->opacus) (12.5.82)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0->opacus) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0->opacus) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import necessary libs\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from opacus import PrivacyEngine\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "5864IOMGZNyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path = '/content/drive/MyDrive/UNI/Sem 6/ML/Project'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJJHh0xkmY4a",
        "outputId": "0322f6da-b030-4801-9ef5-401c092ad383"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Included python file\n",
        "class CIFAR10Classifier(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CIFAR10Classifier, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 16, 3, 1)\n",
        "    self.conv2 = nn.Conv2d(16, 32, 3, 1)\n",
        "    self.dropout1 = nn.Dropout2d(0.25)\n",
        "    self.dropout2 = nn.Dropout2d(0.5)\n",
        "    self.fc1 = nn.Linear(6272, 64)\n",
        "    self.fc2 = nn.Linear(64, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.conv2(x)\n",
        "    x = F.relu(x)\n",
        "    x = F.max_pool2d(x, 2)\n",
        "    x = self.dropout1(x)\n",
        "    x = torch.flatten(x, 1)\n",
        "    x = self.fc1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.dropout2(x)\n",
        "    x = self.fc2(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "PvWXBOm-ZNBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# include CIFAR10 Dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "# Load the CIFAR-10 dataset\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Split the training data into training and validation sets\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Print the number of training, validation, and test samples\n",
        "print(\"Number of training samples:\", len(train_dataset))\n",
        "print(\"Number of validation samples:\", len(val_dataset))\n",
        "print(\"Number of test samples:\", len(test_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUZ-HlkUmhAy",
        "outputId": "21574db2-511d-4af7-94ee-1970cdd05067"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Number of training samples: 40000\n",
            "Number of validation samples: 10000\n",
            "Number of test samples: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function\n",
        "def train(model, device, train_loader, optimizer, criterion, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}'\n",
        "                  f' ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
        "\n",
        "# Validation function\n",
        "def validate(model, device, val_loader, criterion):\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in val_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            val_loss += criterion(output, target).item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    print(f'\\nValidation set: Average loss: {val_loss:.4f}, Accuracy: {correct}/{len(val_loader.dataset)}'\n",
        "          f' ({100. * correct / len(val_loader.dataset):.0f}%)\\n')\n",
        "\n",
        "\n",
        "# Test function (Privacy)\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)}'\n",
        "          f' ({100. * correct / len(test_loader.dataset):.0f}%)\\n')\n",
        "    return 100. * correct / len(test_loader.dataset)"
      ],
      "metadata": {
        "id": "CGV9IcY1nhef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model, define the loss function and the optimizer\n",
        "model = CIFAR10Classifier()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.RMSprop(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "q5CWd593nsgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device (GPU/CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Training and validation loop\n",
        "num_epochs = 15\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    train(model, device, train_loader, optimizer, criterion, epoch)\n",
        "    validate(model, device, val_loader, criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYWM2CJ_nx6w",
        "outputId": "06406b1a-8139-4a79-a281-f634a24fdd1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:1374: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/40000 (0%)]\tLoss: 2.293126\n",
            "Train Epoch: 1 [6400/40000 (16%)]\tLoss: 1.768904\n",
            "Train Epoch: 1 [12800/40000 (32%)]\tLoss: 1.781297\n",
            "Train Epoch: 1 [19200/40000 (48%)]\tLoss: 1.839950\n",
            "Train Epoch: 1 [25600/40000 (64%)]\tLoss: 1.517123\n",
            "Train Epoch: 1 [32000/40000 (80%)]\tLoss: 1.633967\n",
            "Train Epoch: 1 [38400/40000 (96%)]\tLoss: 1.727788\n",
            "\n",
            "Validation set: Average loss: 0.0216, Accuracy: 5193/10000 (52%)\n",
            "\n",
            "Train Epoch: 2 [0/40000 (0%)]\tLoss: 1.537562\n",
            "Train Epoch: 2 [6400/40000 (16%)]\tLoss: 1.423624\n",
            "Train Epoch: 2 [12800/40000 (32%)]\tLoss: 1.391905\n",
            "Train Epoch: 2 [19200/40000 (48%)]\tLoss: 1.475591\n",
            "Train Epoch: 2 [25600/40000 (64%)]\tLoss: 1.458956\n",
            "Train Epoch: 2 [32000/40000 (80%)]\tLoss: 1.433999\n",
            "Train Epoch: 2 [38400/40000 (96%)]\tLoss: 1.397605\n",
            "\n",
            "Validation set: Average loss: 0.0188, Accuracy: 5771/10000 (58%)\n",
            "\n",
            "Train Epoch: 3 [0/40000 (0%)]\tLoss: 1.255708\n",
            "Train Epoch: 3 [6400/40000 (16%)]\tLoss: 1.602806\n",
            "Train Epoch: 3 [12800/40000 (32%)]\tLoss: 0.963266\n",
            "Train Epoch: 3 [19200/40000 (48%)]\tLoss: 1.288574\n",
            "Train Epoch: 3 [25600/40000 (64%)]\tLoss: 1.148816\n",
            "Train Epoch: 3 [32000/40000 (80%)]\tLoss: 1.668317\n",
            "Train Epoch: 3 [38400/40000 (96%)]\tLoss: 1.382322\n",
            "\n",
            "Validation set: Average loss: 0.0179, Accuracy: 6001/10000 (60%)\n",
            "\n",
            "Train Epoch: 4 [0/40000 (0%)]\tLoss: 1.155059\n",
            "Train Epoch: 4 [6400/40000 (16%)]\tLoss: 1.297737\n",
            "Train Epoch: 4 [12800/40000 (32%)]\tLoss: 1.071422\n",
            "Train Epoch: 4 [19200/40000 (48%)]\tLoss: 1.309030\n",
            "Train Epoch: 4 [25600/40000 (64%)]\tLoss: 1.318002\n",
            "Train Epoch: 4 [32000/40000 (80%)]\tLoss: 1.159714\n",
            "Train Epoch: 4 [38400/40000 (96%)]\tLoss: 1.036589\n",
            "\n",
            "Validation set: Average loss: 0.0169, Accuracy: 6310/10000 (63%)\n",
            "\n",
            "Train Epoch: 5 [0/40000 (0%)]\tLoss: 1.190585\n",
            "Train Epoch: 5 [6400/40000 (16%)]\tLoss: 1.118535\n",
            "Train Epoch: 5 [12800/40000 (32%)]\tLoss: 1.345826\n",
            "Train Epoch: 5 [19200/40000 (48%)]\tLoss: 1.341831\n",
            "Train Epoch: 5 [25600/40000 (64%)]\tLoss: 1.186609\n",
            "Train Epoch: 5 [32000/40000 (80%)]\tLoss: 1.518484\n",
            "Train Epoch: 5 [38400/40000 (96%)]\tLoss: 0.960796\n",
            "\n",
            "Validation set: Average loss: 0.0166, Accuracy: 6351/10000 (64%)\n",
            "\n",
            "Train Epoch: 6 [0/40000 (0%)]\tLoss: 1.218783\n",
            "Train Epoch: 6 [6400/40000 (16%)]\tLoss: 0.915048\n",
            "Train Epoch: 6 [12800/40000 (32%)]\tLoss: 1.247183\n",
            "Train Epoch: 6 [19200/40000 (48%)]\tLoss: 1.054760\n",
            "Train Epoch: 6 [25600/40000 (64%)]\tLoss: 1.122432\n",
            "Train Epoch: 6 [32000/40000 (80%)]\tLoss: 1.156287\n",
            "Train Epoch: 6 [38400/40000 (96%)]\tLoss: 1.097963\n",
            "\n",
            "Validation set: Average loss: 0.0161, Accuracy: 6390/10000 (64%)\n",
            "\n",
            "Train Epoch: 7 [0/40000 (0%)]\tLoss: 1.340556\n",
            "Train Epoch: 7 [6400/40000 (16%)]\tLoss: 1.153353\n",
            "Train Epoch: 7 [12800/40000 (32%)]\tLoss: 1.206593\n",
            "Train Epoch: 7 [19200/40000 (48%)]\tLoss: 1.139391\n",
            "Train Epoch: 7 [25600/40000 (64%)]\tLoss: 1.108644\n",
            "Train Epoch: 7 [32000/40000 (80%)]\tLoss: 1.219061\n",
            "Train Epoch: 7 [38400/40000 (96%)]\tLoss: 0.994327\n",
            "\n",
            "Validation set: Average loss: 0.0160, Accuracy: 6456/10000 (65%)\n",
            "\n",
            "Train Epoch: 8 [0/40000 (0%)]\tLoss: 0.977925\n",
            "Train Epoch: 8 [6400/40000 (16%)]\tLoss: 1.046865\n",
            "Train Epoch: 8 [12800/40000 (32%)]\tLoss: 1.205415\n",
            "Train Epoch: 8 [19200/40000 (48%)]\tLoss: 1.066968\n",
            "Train Epoch: 8 [25600/40000 (64%)]\tLoss: 1.217731\n",
            "Train Epoch: 8 [32000/40000 (80%)]\tLoss: 0.999156\n",
            "Train Epoch: 8 [38400/40000 (96%)]\tLoss: 1.506637\n",
            "\n",
            "Validation set: Average loss: 0.0158, Accuracy: 6584/10000 (66%)\n",
            "\n",
            "Train Epoch: 9 [0/40000 (0%)]\tLoss: 1.023442\n",
            "Train Epoch: 9 [6400/40000 (16%)]\tLoss: 1.218537\n",
            "Train Epoch: 9 [12800/40000 (32%)]\tLoss: 1.115108\n",
            "Train Epoch: 9 [19200/40000 (48%)]\tLoss: 1.121944\n",
            "Train Epoch: 9 [25600/40000 (64%)]\tLoss: 1.066157\n",
            "Train Epoch: 9 [32000/40000 (80%)]\tLoss: 1.007409\n",
            "Train Epoch: 9 [38400/40000 (96%)]\tLoss: 0.913524\n",
            "\n",
            "Validation set: Average loss: 0.0157, Accuracy: 6560/10000 (66%)\n",
            "\n",
            "Train Epoch: 10 [0/40000 (0%)]\tLoss: 0.929383\n",
            "Train Epoch: 10 [6400/40000 (16%)]\tLoss: 1.025300\n",
            "Train Epoch: 10 [12800/40000 (32%)]\tLoss: 1.207501\n",
            "Train Epoch: 10 [19200/40000 (48%)]\tLoss: 1.102766\n",
            "Train Epoch: 10 [25600/40000 (64%)]\tLoss: 1.177062\n",
            "Train Epoch: 10 [32000/40000 (80%)]\tLoss: 0.990349\n",
            "Train Epoch: 10 [38400/40000 (96%)]\tLoss: 0.947757\n",
            "\n",
            "Validation set: Average loss: 0.0156, Accuracy: 6528/10000 (65%)\n",
            "\n",
            "Train Epoch: 11 [0/40000 (0%)]\tLoss: 0.817207\n",
            "Train Epoch: 11 [6400/40000 (16%)]\tLoss: 1.243294\n",
            "Train Epoch: 11 [12800/40000 (32%)]\tLoss: 0.972356\n",
            "Train Epoch: 11 [19200/40000 (48%)]\tLoss: 0.973392\n",
            "Train Epoch: 11 [25600/40000 (64%)]\tLoss: 1.207320\n",
            "Train Epoch: 11 [32000/40000 (80%)]\tLoss: 0.963745\n",
            "Train Epoch: 11 [38400/40000 (96%)]\tLoss: 0.848720\n",
            "\n",
            "Validation set: Average loss: 0.0155, Accuracy: 6579/10000 (66%)\n",
            "\n",
            "Train Epoch: 12 [0/40000 (0%)]\tLoss: 1.077850\n",
            "Train Epoch: 12 [6400/40000 (16%)]\tLoss: 1.356472\n",
            "Train Epoch: 12 [12800/40000 (32%)]\tLoss: 1.059090\n",
            "Train Epoch: 12 [19200/40000 (48%)]\tLoss: 1.117620\n",
            "Train Epoch: 12 [25600/40000 (64%)]\tLoss: 0.986442\n",
            "Train Epoch: 12 [32000/40000 (80%)]\tLoss: 0.885808\n",
            "Train Epoch: 12 [38400/40000 (96%)]\tLoss: 1.384022\n",
            "\n",
            "Validation set: Average loss: 0.0154, Accuracy: 6605/10000 (66%)\n",
            "\n",
            "Train Epoch: 13 [0/40000 (0%)]\tLoss: 0.933716\n",
            "Train Epoch: 13 [6400/40000 (16%)]\tLoss: 1.027425\n",
            "Train Epoch: 13 [12800/40000 (32%)]\tLoss: 0.925846\n",
            "Train Epoch: 13 [19200/40000 (48%)]\tLoss: 0.767384\n",
            "Train Epoch: 13 [25600/40000 (64%)]\tLoss: 1.139744\n",
            "Train Epoch: 13 [32000/40000 (80%)]\tLoss: 0.736116\n",
            "Train Epoch: 13 [38400/40000 (96%)]\tLoss: 0.951662\n",
            "\n",
            "Validation set: Average loss: 0.0154, Accuracy: 6616/10000 (66%)\n",
            "\n",
            "Train Epoch: 14 [0/40000 (0%)]\tLoss: 0.675812\n",
            "Train Epoch: 14 [6400/40000 (16%)]\tLoss: 1.033471\n",
            "Train Epoch: 14 [12800/40000 (32%)]\tLoss: 0.879828\n",
            "Train Epoch: 14 [19200/40000 (48%)]\tLoss: 1.121968\n",
            "Train Epoch: 14 [25600/40000 (64%)]\tLoss: 0.874833\n",
            "Train Epoch: 14 [32000/40000 (80%)]\tLoss: 1.165445\n",
            "Train Epoch: 14 [38400/40000 (96%)]\tLoss: 1.356211\n",
            "\n",
            "Validation set: Average loss: 0.0157, Accuracy: 6500/10000 (65%)\n",
            "\n",
            "Train Epoch: 15 [0/40000 (0%)]\tLoss: 0.662450\n",
            "Train Epoch: 15 [6400/40000 (16%)]\tLoss: 1.034700\n",
            "Train Epoch: 15 [12800/40000 (32%)]\tLoss: 0.976906\n",
            "Train Epoch: 15 [19200/40000 (48%)]\tLoss: 1.002746\n",
            "Train Epoch: 15 [25600/40000 (64%)]\tLoss: 0.938375\n",
            "Train Epoch: 15 [32000/40000 (80%)]\tLoss: 0.674532\n",
            "Train Epoch: 15 [38400/40000 (96%)]\tLoss: 1.140915\n",
            "\n",
            "Validation set: Average loss: 0.0155, Accuracy: 6602/10000 (66%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "torch.save(model.state_dict(), \"cifar10_baseline_model.pth\")\n",
        "baseline_accuracy = test(model, device, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REvELAgFpx0f",
        "outputId": "3a6ee64e-61b8-4b11-8686-03b5614c38e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0153, Accuracy: 6579/10000 (66%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Private Training - Question 5"
      ],
      "metadata": {
        "id": "UPu2rQ8OWoWz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from opacus.validators import ModuleValidator\n",
        "model_pr = CIFAR10Classifier()\n",
        "\n",
        "\n",
        "errors = ModuleValidator.validate(model_pr, strict=False)\n",
        "errors[-5:]\n",
        "\n",
        "model_pr = ModuleValidator.fix(model_pr)\n",
        "ModuleValidator.validate(model_pr, strict=False)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = model_pr.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "LR = 0.001\n",
        "optimizer = optim.RMSprop(model_pr.parameters(), lr=LR)\n"
      ],
      "metadata": {
        "id": "b0v_W3WuR4qJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from opacus import PrivacyEngine\n",
        "MAX_GRAD_NORM = 1.2\n",
        "EPSILON = 50.0\n",
        "DELTA = 1e-5\n",
        "EPOCHS = 15\n",
        "BATCH_SIZE = 512\n",
        "MAX_PHYSICAL_BATCH_SIZE = 128\n",
        "LR = 1e-3\n",
        "privacy_engine = PrivacyEngine()\n",
        "\n",
        "model_pr, optimizer, train_loader = privacy_engine.make_private_with_epsilon(\n",
        "    module=model_pr,\n",
        "    optimizer=optimizer,\n",
        "    data_loader=train_loader,\n",
        "    epochs=EPOCHS,\n",
        "    target_epsilon=EPSILON,\n",
        "    target_delta=DELTA,\n",
        "    max_grad_norm=MAX_GRAD_NORM,\n",
        ")\n",
        "\n",
        "print(f\"Using sigma={optimizer.noise_multiplier} and C={MAX_GRAD_NORM}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVjL4dxhVhbI",
        "outputId": "85a53949-4e55-4b19-a8fe-dea288b29fe2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the smallest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: overflow encountered in exp\n",
            "  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))\n",
            "/usr/local/lib/python3.10/dist-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: overflow encountered in divide\n",
            "  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))\n",
            "/usr/local/lib/python3.10/dist-packages/opacus/accountants/analysis/prv/prvs.py:52: RuntimeWarning: overflow encountered in scalar divide\n",
            "  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))\n",
            "/usr/local/lib/python3.10/dist-packages/opacus/accountants/analysis/prv/prvs.py:145: RuntimeWarning: overflow encountered in exp\n",
            "  d2 = np.flip(np.flip(p * np.exp(-t)).cumsum())\n",
            "/usr/local/lib/python3.10/dist-packages/opacus/accountants/analysis/prv/prvs.py:146: RuntimeWarning: overflow encountered in exp\n",
            "  ndelta = np.exp(t) * d2 - d1\n",
            "/usr/local/lib/python3.10/dist-packages/opacus/accountants/analysis/prv/prvs.py:146: RuntimeWarning: invalid value encountered in multiply\n",
            "  ndelta = np.exp(t) * d2 - d1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using sigma=0.3096771240234375 and C=1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(preds, labels):\n",
        "    return (preds == labels).mean()\n",
        "\n",
        "import numpy as np\n",
        "from opacus.utils.batch_memory_manager import BatchMemoryManager\n",
        "\n",
        "\n",
        "def train(model, train_loader, optimizer, epoch, device):\n",
        "    model.train()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    losses = []\n",
        "    top1_acc = []\n",
        "\n",
        "    with BatchMemoryManager(\n",
        "        data_loader=train_loader,\n",
        "        max_physical_batch_size=MAX_PHYSICAL_BATCH_SIZE,\n",
        "        optimizer=optimizer\n",
        "    ) as memory_safe_data_loader:\n",
        "\n",
        "        for i, (images, target) in enumerate(memory_safe_data_loader):\n",
        "            optimizer.zero_grad()\n",
        "            images = images.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            # compute output\n",
        "            output = model(images)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            preds = np.argmax(output.detach().cpu().numpy(), axis=1)\n",
        "            labels = target.detach().cpu().numpy()\n",
        "\n",
        "            # measure accuracy and record loss\n",
        "            acc = accuracy(preds, labels)\n",
        "\n",
        "            losses.append(loss.item())\n",
        "            top1_acc.append(acc)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if (i+1) % 200 == 0:\n",
        "                epsilon = privacy_engine.get_epsilon(DELTA)\n",
        "                print(\n",
        "                    f\"\\tTrain Epoch: {epoch} \\t\"\n",
        "                    f\"Loss: {np.mean(losses):.6f} \"\n",
        "                    f\"Acc@1: {np.mean(top1_acc) * 100:.6f} \"\n",
        "                    f\"(ε = {epsilon:.2f}, δ = {DELTA})\"\n",
        "                )\n",
        "def test(model, test_loader, device):\n",
        "    model.eval()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    losses = []\n",
        "    top1_acc = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, target in test_loader:\n",
        "            images = images.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            output = model(images)\n",
        "            loss = criterion(output, target)\n",
        "            preds = np.argmax(output.detach().cpu().numpy(), axis=1)\n",
        "            labels = target.detach().cpu().numpy()\n",
        "            acc = accuracy(preds, labels)\n",
        "\n",
        "            losses.append(loss.item())\n",
        "            top1_acc.append(acc)\n",
        "\n",
        "    top1_avg = np.mean(top1_acc)\n",
        "\n",
        "    print(\n",
        "        f\"\\tTest set:\"\n",
        "        f\"Loss: {np.mean(losses):.6f} \"\n",
        "        f\"Acc: {top1_avg * 100:.6f} \"\n",
        "    )\n",
        "    return np.mean(top1_acc)"
      ],
      "metadata": {
        "id": "Ocdf9JxfVrP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "for epoch in tqdm(range(EPOCHS), desc=\"Epoch\", unit=\"epoch\"):\n",
        "    train(model_pr, train_loader, optimizer, epoch + 1, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 885,
          "referenced_widgets": [
            "0b29fee416ec4603b352996738065f3e",
            "10c3f1c5621a4a8eac3f096a3bace128",
            "59f9e510c3094e049033a83d19d5d748",
            "3d63a47c46c14a8b9dfc7ebff5db9498",
            "208c7b82077c45d3aaf0d686c7e7e512",
            "d50bcdd7eec449d38fa7666afcbfabe4",
            "9ce287b19f794d13820e4301b645ad0d",
            "824abfbb73b74df0954a6fcc7f481bcf",
            "513b63052f5942b5a0cce75b7954b4d0",
            "99664359c2324868ba2c3fb22cbbd4c9",
            "da2bfc12d1af469db0185e293a906a52"
          ]
        },
        "id": "-07LL6pbWGXE",
        "outputId": "d0336bd1-2b26-47ce-c9a5-a198b1772f3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch:   0%|          | 0/15 [00:00<?, ?epoch/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0b29fee416ec4603b352996738065f3e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTrain Epoch: 1 \tLoss: 2.116126 Acc@1: 22.440465 (ε = 13.52, δ = 1e-05)\n",
            "\tTrain Epoch: 1 \tLoss: 2.071803 Acc@1: 24.792369 (ε = 15.89, δ = 1e-05)\n",
            "\tTrain Epoch: 1 \tLoss: 2.049935 Acc@1: 26.383371 (ε = 17.67, δ = 1e-05)\n",
            "\tTrain Epoch: 2 \tLoss: 1.984687 Acc@1: 31.004076 (ε = 19.35, δ = 1e-05)\n",
            "\tTrain Epoch: 2 \tLoss: 1.975376 Acc@1: 31.810454 (ε = 20.66, δ = 1e-05)\n",
            "\tTrain Epoch: 2 \tLoss: 1.968866 Acc@1: 32.324591 (ε = 21.86, δ = 1e-05)\n",
            "\tTrain Epoch: 3 \tLoss: 1.943159 Acc@1: 34.529360 (ε = 23.11, δ = 1e-05)\n",
            "\tTrain Epoch: 3 \tLoss: 1.942366 Acc@1: 34.611096 (ε = 24.14, δ = 1e-05)\n",
            "\tTrain Epoch: 3 \tLoss: 1.942284 Acc@1: 34.700264 (ε = 25.13, δ = 1e-05)\n",
            "\tTrain Epoch: 4 \tLoss: 1.926930 Acc@1: 36.606448 (ε = 26.19, δ = 1e-05)\n",
            "\tTrain Epoch: 4 \tLoss: 1.925441 Acc@1: 36.092788 (ε = 27.09, δ = 1e-05)\n",
            "\tTrain Epoch: 4 \tLoss: 1.934675 Acc@1: 36.196113 (ε = 27.95, δ = 1e-05)\n",
            "\tTrain Epoch: 5 \tLoss: 1.934931 Acc@1: 37.044004 (ε = 28.90, δ = 1e-05)\n",
            "\tTrain Epoch: 5 \tLoss: 1.935749 Acc@1: 36.998369 (ε = 29.71, δ = 1e-05)\n",
            "\tTrain Epoch: 5 \tLoss: 1.933865 Acc@1: 36.989115 (ε = 30.50, δ = 1e-05)\n",
            "\tTrain Epoch: 6 \tLoss: 1.963227 Acc@1: 37.121316 (ε = 31.37, δ = 1e-05)\n",
            "\tTrain Epoch: 6 \tLoss: 1.949935 Acc@1: 37.257106 (ε = 32.12, δ = 1e-05)\n",
            "\tTrain Epoch: 6 \tLoss: 1.946754 Acc@1: 37.444613 (ε = 32.86, δ = 1e-05)\n",
            "\tTrain Epoch: 7 \tLoss: 1.950881 Acc@1: 38.574261 (ε = 33.67, δ = 1e-05)\n",
            "\tTrain Epoch: 7 \tLoss: 1.957544 Acc@1: 38.168778 (ε = 34.37, δ = 1e-05)\n",
            "\tTrain Epoch: 7 \tLoss: 1.948125 Acc@1: 38.384184 (ε = 35.07, δ = 1e-05)\n",
            "\tTrain Epoch: 8 \tLoss: 1.966502 Acc@1: 38.093500 (ε = 35.83, δ = 1e-05)\n",
            "\tTrain Epoch: 8 \tLoss: 1.951431 Acc@1: 38.110580 (ε = 36.50, δ = 1e-05)\n",
            "\tTrain Epoch: 8 \tLoss: 1.946361 Acc@1: 38.179134 (ε = 37.16, δ = 1e-05)\n",
            "\tTrain Epoch: 9 \tLoss: 1.947127 Acc@1: 38.676969 (ε = 37.89, δ = 1e-05)\n",
            "\tTrain Epoch: 9 \tLoss: 1.943074 Acc@1: 38.652332 (ε = 38.53, δ = 1e-05)\n",
            "\tTrain Epoch: 9 \tLoss: 1.944561 Acc@1: 38.671716 (ε = 39.17, δ = 1e-05)\n",
            "\tTrain Epoch: 10 \tLoss: 1.950974 Acc@1: 38.059496 (ε = 39.87, δ = 1e-05)\n",
            "\tTrain Epoch: 10 \tLoss: 1.936809 Acc@1: 39.042599 (ε = 40.49, δ = 1e-05)\n",
            "\tTrain Epoch: 10 \tLoss: 1.936707 Acc@1: 39.239390 (ε = 41.10, δ = 1e-05)\n",
            "\tTrain Epoch: 11 \tLoss: 1.930052 Acc@1: 38.934972 (ε = 41.78, δ = 1e-05)\n",
            "\tTrain Epoch: 11 \tLoss: 1.939532 Acc@1: 39.189345 (ε = 42.37, δ = 1e-05)\n",
            "\tTrain Epoch: 11 \tLoss: 1.944498 Acc@1: 39.434007 (ε = 42.96, δ = 1e-05)\n",
            "\tTrain Epoch: 12 \tLoss: 1.956064 Acc@1: 38.933088 (ε = 43.62, δ = 1e-05)\n",
            "\tTrain Epoch: 12 \tLoss: 1.967137 Acc@1: 39.435958 (ε = 44.20, δ = 1e-05)\n",
            "\tTrain Epoch: 12 \tLoss: 1.959348 Acc@1: 39.545785 (ε = 44.77, δ = 1e-05)\n",
            "\tTrain Epoch: 13 \tLoss: 1.952839 Acc@1: 40.434597 (ε = 45.41, δ = 1e-05)\n",
            "\tTrain Epoch: 13 \tLoss: 1.954162 Acc@1: 40.213509 (ε = 45.97, δ = 1e-05)\n",
            "\tTrain Epoch: 13 \tLoss: 1.957693 Acc@1: 39.875494 (ε = 46.53, δ = 1e-05)\n",
            "\tTrain Epoch: 14 \tLoss: 1.954039 Acc@1: 39.550549 (ε = 47.15, δ = 1e-05)\n",
            "\tTrain Epoch: 14 \tLoss: 1.935846 Acc@1: 40.040961 (ε = 47.70, δ = 1e-05)\n",
            "\tTrain Epoch: 14 \tLoss: 1.948157 Acc@1: 40.114792 (ε = 48.25, δ = 1e-05)\n",
            "\tTrain Epoch: 15 \tLoss: 1.969815 Acc@1: 40.119451 (ε = 48.86, δ = 1e-05)\n",
            "\tTrain Epoch: 15 \tLoss: 1.968631 Acc@1: 40.405159 (ε = 49.40, δ = 1e-05)\n",
            "\tTrain Epoch: 15 \tLoss: 1.958824 Acc@1: 40.312545 (ε = 49.93, δ = 1e-05)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top1_acc = test(model_pr, test_loader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_YWET4oWQWg",
        "outputId": "dd739375-7d16-4720-ef29-fa7ad4a461c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTest set:Loss: 1.533106 Acc: 47.133758 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "# Load previously trained models (replace with actual model paths)\n",
        "baseline_model = model\n",
        "private_model = model_pr\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "baseline_model.to(device)\n",
        "private_model.to(device)\n",
        "\n",
        "# Function to get outputs and labels\n",
        "def get_outputs_and_labels(model, loader, device, label):\n",
        "    model.eval()\n",
        "    outputs = []\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "        for data, _ in loader:\n",
        "            data = data.to(device)\n",
        "            output = model(data)\n",
        "            outputs.append(output.cpu().numpy())\n",
        "            labels.extend([label] * data.size(0))\n",
        "    outputs = np.concatenate(outputs, axis=0)\n",
        "    return outputs, labels\n",
        "\n",
        "# Generate membership labels and outputs for attacker models\n",
        "# Seen data: label 1\n",
        "seen_outputs_baseline, seen_labels_baseline = get_outputs_and_labels(baseline_model, train_loader, device, 1)\n",
        "seen_outputs_private, seen_labels_private = get_outputs_and_labels(private_model, train_loader, device, 1)\n",
        "\n",
        "# Unseen data: label 0\n",
        "unseen_outputs_baseline, unseen_labels_baseline = get_outputs_and_labels(baseline_model, val_loader, device, 0)\n",
        "unseen_outputs_baseline_test, unseen_labels_baseline_test = get_outputs_and_labels(baseline_model, test_loader, device, 0)\n",
        "\n",
        "unseen_outputs_private, unseen_labels_private = get_outputs_and_labels(private_model, val_loader, device, 0)\n",
        "unseen_outputs_private_test, unseen_labels_private_test = get_outputs_and_labels(private_model, test_loader, device, 0)\n",
        "\n",
        "# Combine seen and unseen data\n",
        "baseline_outputs = np.vstack([seen_outputs_baseline, unseen_outputs_baseline, unseen_outputs_baseline_test])\n",
        "baseline_labels = seen_labels_baseline + unseen_labels_baseline + unseen_labels_baseline_test\n",
        "private_outputs = np.vstack([seen_outputs_private, unseen_outputs_private, unseen_outputs_private_test])\n",
        "private_labels = seen_labels_private + unseen_labels_private + unseen_labels_private_test"
      ],
      "metadata": {
        "id": "tkdXan-pqduo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define and train attacker models\n",
        "class AttackerModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AttackerModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(10, 50)\n",
        "        self.fc2 = nn.Linear(50, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "attacker_baseline = AttackerModel().to(device)\n",
        "attacker_private = AttackerModel().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_attacker_baseline = optim.Adam(attacker_baseline.parameters(), lr=0.001)\n",
        "optimizer_attacker_private = optim.Adam(attacker_private.parameters(), lr=0.001)\n",
        "\n",
        "def train_attacker(model, data, labels, optimizer, epochs=10):\n",
        "    model.train()\n",
        "    data = torch.tensor(data, dtype=torch.float32).to(device)\n",
        "    labels = torch.tensor(labels, dtype=torch.long).to(device)\n",
        "    dataset = torch.utils.data.TensorDataset(data, labels)\n",
        "    loader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for inputs, targets in loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(f'Epoch {epoch + 1}, Loss: {loss.item()}')\n",
        "\n",
        "train_attacker(attacker_baseline, baseline_outputs, baseline_labels, optimizer_attacker_baseline)\n",
        "train_attacker(attacker_private, private_outputs, private_labels, optimizer_attacker_private)\n",
        "\n",
        "# Evaluate attacker models\n",
        "def evaluate_attacker(model, data, labels):\n",
        "    model.eval()\n",
        "    data = torch.tensor(data, dtype=torch.float32).to(device)\n",
        "    labels = torch.tensor(labels, dtype=torch.long).to(device)\n",
        "    dataset = torch.utils.data.TensorDataset(data, labels)\n",
        "    loader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in loader:\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += torch.sum(preds == targets).item()\n",
        "    accuracy = correct / len(labels)\n",
        "    return accuracy\n",
        "\n",
        "attacker_baseline_acc = evaluate_attacker(attacker_baseline, baseline_outputs, baseline_labels)\n",
        "attacker_private_acc = evaluate_attacker(attacker_private, private_outputs, private_labels)\n",
        "\n",
        "print(f'Attacker Baseline Model Accuracy: {attacker_baseline_acc * 100:.2f}%')\n",
        "print(f'Attacker Private Model Accuracy: {attacker_private_acc * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2OOqGPMtCHU",
        "outputId": "f62c676d-df21-40b8-db43-e0da094b8393"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.5299001336097717\n",
            "Epoch 2, Loss: 0.551960825920105\n",
            "Epoch 3, Loss: 0.6493751406669617\n",
            "Epoch 4, Loss: 0.5085877180099487\n",
            "Epoch 5, Loss: 0.663835883140564\n",
            "Epoch 6, Loss: 0.6811718940734863\n",
            "Epoch 7, Loss: 0.4822832942008972\n",
            "Epoch 8, Loss: 0.9089839458465576\n",
            "Epoch 9, Loss: 0.6010143756866455\n",
            "Epoch 10, Loss: 0.664049506187439\n",
            "Epoch 1, Loss: 0.7566581964492798\n",
            "Epoch 2, Loss: 0.6658103466033936\n",
            "Epoch 3, Loss: 0.6139968633651733\n",
            "Epoch 4, Loss: 0.6689863801002502\n",
            "Epoch 5, Loss: 0.6054378151893616\n",
            "Epoch 6, Loss: 0.6059554815292358\n",
            "Epoch 7, Loss: 0.6695582866668701\n",
            "Epoch 8, Loss: 0.6558816432952881\n",
            "Epoch 9, Loss: 0.6339205503463745\n",
            "Epoch 10, Loss: 0.6222830414772034\n",
            "Attacker Baseline Model Accuracy: 66.72%\n",
            "Attacker Private Model Accuracy: 66.53%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import numpy as np\n",
        "\n",
        "baseline_model = model\n",
        "private_model = model_pr\n",
        "# baseline_model.load_state_dict(torch.load('baseline_model.pth'))\n",
        "# private_model.load_state_dict(torch.load('private_model.pth'))\n",
        "\n",
        "# Define device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "baseline_model.to(device)\n",
        "private_model.to(device)\n",
        "\n",
        "# Function to train a shadow model\n",
        "def train_shadow_model(data_loader, model, optimizer, criterion, epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        for data, target in data_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "# Create shadow datasets\n",
        "def create_shadow_datasets(train_dataset, num_shadow_models=5):\n",
        "    shadow_datasets = []\n",
        "    size = len(train_dataset) // num_shadow_models\n",
        "    for _ in range(num_shadow_models):\n",
        "        indices = torch.randperm(len(train_dataset))[:size]\n",
        "        shadow_data = torch.utils.data.Subset(train_dataset, indices)\n",
        "        shadow_datasets.append(shadow_data)\n",
        "    return shadow_datasets\n",
        "\n",
        "# Train shadow models\n",
        "num_shadow_models = 5\n",
        "shadow_datasets = create_shadow_datasets(train_dataset, num_shadow_models)\n",
        "\n",
        "shadow_models_baseline = [CIFAR10Classifier().to(device) for _ in range(num_shadow_models)]\n",
        "# shadow_models_private = [CIFAR10Classifier().to(device) for _ in range(num_shadow_models)]\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for model in shadow_models_baseline:\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    shadow_loader = DataLoader(shadow_datasets.pop(), batch_size=64, shuffle=True)\n",
        "    train_shadow_model(shadow_loader, model, optimizer, criterion)\n",
        "\n",
        "shadow_datasets_private = create_shadow_datasets(train_dataset, num_shadow_models)\n",
        "shadow_models_private = [CIFAR10Classifier().to(device) for _ in range(num_shadow_models)]\n",
        "\n",
        "for model, shadow_data in zip(shadow_models_private, shadow_datasets_private):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    shadow_loader = DataLoader(shadow_data, batch_size=64, shuffle=True)\n",
        "    train_shadow_model(shadow_loader, model, optimizer, criterion)\n",
        "\n",
        "# Function to get outputs and labels from shadow models\n",
        "def get_shadow_outputs_and_labels(models, loader, label):\n",
        "    all_outputs = []\n",
        "    all_labels = []\n",
        "    for model in models:\n",
        "        model.eval()\n",
        "        outputs = []\n",
        "        labels = []\n",
        "        with torch.no_grad():\n",
        "            for data, _ in loader:\n",
        "                data = data.to(device)\n",
        "                output = model(data)\n",
        "                outputs.append(output.cpu().numpy())\n",
        "                labels.extend([label] * data.size(0))\n",
        "        outputs = np.concatenate(outputs, axis=0)\n",
        "        all_outputs.append(outputs)\n",
        "        all_labels.extend(labels)\n",
        "    all_outputs = np.concatenate(all_outputs, axis=0)\n",
        "    return all_outputs, all_labels\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOjPULt6upSy",
        "outputId": "a020f6c6-a851-41fc-b1a3-45428249abc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.5297827124595642\n",
            "Epoch 2, Loss: 0.667941153049469\n",
            "Epoch 3, Loss: 0.5147463083267212\n",
            "Epoch 4, Loss: 0.6013748049736023\n",
            "Epoch 5, Loss: 0.6876029372215271\n",
            "Epoch 6, Loss: 0.6450225710868835\n",
            "Epoch 7, Loss: 0.6657246351242065\n",
            "Epoch 8, Loss: 0.5786383748054504\n",
            "Epoch 9, Loss: 0.6009915471076965\n",
            "Epoch 10, Loss: 0.6212524771690369\n",
            "Epoch 1, Loss: 0.6416228413581848\n",
            "Epoch 2, Loss: 0.6235588192939758\n",
            "Epoch 3, Loss: 0.5773477554321289\n",
            "Epoch 4, Loss: 0.5311208367347717\n",
            "Epoch 5, Loss: 0.5373037457466125\n",
            "Epoch 6, Loss: 0.5162485837936401\n",
            "Epoch 7, Loss: 0.6883094310760498\n",
            "Epoch 8, Loss: 0.6218485236167908\n",
            "Epoch 9, Loss: 0.6208744049072266\n",
            "Epoch 10, Loss: 0.6434410810470581\n",
            "Attacker Baseline Model Accuracy: 66.67%\n",
            "Attacker Private Model Accuracy: 66.89%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate membership labels and outputs for attacker models using shadow models\n",
        "# Seen data: label 1\n",
        "seen_outputs_shadow_baseline, seen_labels_shadow_baseline = get_shadow_outputs_and_labels(shadow_models_baseline, train_loader, 1)\n",
        "seen_outputs_shadow_private, seen_labels_shadow_private = get_shadow_outputs_and_labels(shadow_models_private, train_loader, 1)\n",
        "\n",
        "# Unseen data: label 0\n",
        "unseen_outputs_shadow_baseline, unseen_labels_shadow_baseline = get_shadow_outputs_and_labels(shadow_models_baseline, val_loader, 0)\n",
        "unseen_outputs_shadow_baseline_test, unseen_labels_shadow_baseline_test = get_shadow_outputs_and_labels(shadow_models_baseline, test_loader, 0)\n",
        "\n",
        "unseen_outputs_shadow_private, unseen_labels_shadow_private = get_shadow_outputs_and_labels(shadow_models_private, val_loader, 0)\n",
        "unseen_outputs_shadow_private_test, unseen_labels_shadow_private_test = get_shadow_outputs_and_labels(shadow_models_private, test_loader, 0)\n",
        "\n",
        "# Combine seen and unseen data for shadow models\n",
        "baseline_outputs_shadow = np.vstack([seen_outputs_shadow_baseline, unseen_outputs_shadow_baseline, unseen_outputs_shadow_baseline_test])\n",
        "baseline_labels_shadow = seen_labels_shadow_baseline + unseen_labels_shadow_baseline + unseen_labels_shadow_baseline_test\n",
        "private_outputs_shadow = np.vstack([seen_outputs_shadow_private, unseen_outputs_shadow_private, unseen_outputs_shadow_private_test])\n",
        "private_labels_shadow = seen_labels_shadow_private + unseen_labels_shadow_private + unseen_labels_shadow_private_test\n",
        "\n",
        "# Define and train attacker models using shadow model data\n",
        "class ImprovedAttackerModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImprovedAttackerModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(10, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 32)\n",
        "        self.fc4 = nn.Linear(32, 2)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "attacker_baseline = ImprovedAttackerModel().to(device)\n",
        "attacker_private = ImprovedAttackerModel().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_attacker_baseline = optim.Adam(attacker_baseline.parameters(), lr=0.001)\n",
        "optimizer_attacker_private = optim.Adam(attacker_private.parameters(), lr=0.001)\n",
        "\n",
        "def train_attacker(model, data, labels, optimizer, epochs=10):\n",
        "    model.train()\n",
        "    data = torch.tensor(data, dtype=torch.float32).to(device)\n",
        "    labels = torch.tensor(labels, dtype=torch.long).to(device)\n",
        "    dataset = torch.utils.data.TensorDataset(data, labels)\n",
        "    loader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for inputs, targets in loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(f'Epoch {epoch + 1}, Loss: {loss.item()}')\n",
        "\n",
        "train_attacker(attacker_baseline, baseline_outputs_shadow, baseline_labels_shadow, optimizer_attacker_baseline)\n",
        "train_attacker(attacker_private, private_outputs_shadow, private_labels_shadow, optimizer_attacker_private)\n",
        "\n",
        "# Evaluate attacker models\n",
        "def evaluate_attacker(model, data, labels):\n",
        "    model.eval()\n",
        "    data = torch.tensor(data, dtype=torch.float32).to(device)\n",
        "    labels = torch.tensor(labels, dtype=torch.long).to(device)\n",
        "    dataset = torch.utils.data.TensorDataset(data, labels)\n",
        "    loader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in loader:\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += torch.sum(preds == targets).item()\n",
        "    accuracy = correct / len(labels)\n",
        "    return accuracy\n",
        "\n",
        "attacker_baseline_acc = evaluate_attacker(attacker_baseline, baseline_outputs_shadow, baseline_labels_shadow)\n",
        "attacker_private_acc =  evaluate_attacker(attacker_private, private_outputs, private_labels)\n",
        "\n",
        "print(f'Attacker Baseline Model Accuracy: {attacker_baseline_acc * 100:.2f}%')\n",
        "print(f'Attacker Private Model Accuracy: {attacker_private_acc * 100:.2f}%')\n",
        "\n"
      ],
      "metadata": {
        "id": "h2J0kz_K0eQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImprovedAttackerModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImprovedAttackerModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(10, 256)\n",
        "        self.bn1 = nn.BatchNorm1d(256)  # Add BatchNorm layer\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.bn2 = nn.BatchNorm1d(128)  # Add BatchNorm layer\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.fc4 = nn.Linear(64, 2)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.fc1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.bn2(self.fc2(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "attacker_baseline = ImprovedAttackerModel().to(device)\n",
        "attacker_private = ImprovedAttackerModel().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_attacker_baseline = optim.Adam(attacker_baseline.parameters(), lr=0.001)\n",
        "optimizer_attacker_private = optim.Adam(attacker_private.parameters(), lr=0.001)\n",
        "\n",
        "def train_attacker(model, data, labels, optimizer, epochs=10):\n",
        "    model.train()\n",
        "    data = torch.tensor(data, dtype=torch.float32).to(device)\n",
        "    labels = torch.tensor(labels, dtype=torch.long).to(device)\n",
        "    dataset = torch.utils.data.TensorDataset(data, labels)\n",
        "    loader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for inputs, targets in loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(f'Epoch {epoch + 1}, Loss: {loss.item()}')\n",
        "\n",
        "train_attacker(attacker_baseline, baseline_outputs_shadow, baseline_labels_shadow, optimizer_attacker_baseline)\n",
        "train_attacker(attacker_private, private_outputs_shadow, private_labels_shadow, optimizer_attacker_private)\n",
        "\n",
        "# Evaluate attacker models\n",
        "def evaluate_attacker(model, data, labels):\n",
        "    model.eval()\n",
        "    data = torch.tensor(data, dtype=torch.float32).to(device)\n",
        "    labels = torch.tensor(labels, dtype=torch.long).to(device)\n",
        "    dataset = torch.utils.data.TensorDataset(data, labels)\n",
        "    loader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in loader:\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += torch.sum(preds == targets).item()\n",
        "    accuracy = correct / len(labels)\n",
        "    return accuracy\n",
        "\n",
        "attacker_baseline_acc = evaluate_attacker(attacker_baseline, baseline_outputs_shadow, baseline_labels_shadow)\n",
        "attacker_private_acc =  evaluate_attacker(attacker_private, private_outputs, private_labels)\n",
        "\n",
        "print(f'Attacker Baseline Model Accuracy: {attacker_baseline_acc * 100:.2f}%')\n",
        "print(f'Attacker Private Model Accuracy: {attacker_private_acc * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ig_G7luY1XuK",
        "outputId": "e4ad8e24-3c46-4caf-b40b-673ee9a42aaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.6331300139427185\n",
            "Epoch 2, Loss: 0.7069498896598816\n",
            "Epoch 3, Loss: 0.6448447704315186\n",
            "Epoch 4, Loss: 0.7074538469314575\n",
            "Epoch 5, Loss: 0.6209133863449097\n",
            "Epoch 6, Loss: 0.6248080134391785\n",
            "Epoch 7, Loss: 0.5333105325698853\n",
            "Epoch 8, Loss: 0.7056225538253784\n",
            "Epoch 9, Loss: 0.641893208026886\n",
            "Epoch 10, Loss: 0.5518613457679749\n",
            "Epoch 1, Loss: 0.6261704564094543\n",
            "Epoch 2, Loss: 0.6842180490493774\n",
            "Epoch 3, Loss: 0.5571067333221436\n",
            "Epoch 4, Loss: 0.6878211498260498\n",
            "Epoch 5, Loss: 0.6012611389160156\n",
            "Epoch 6, Loss: 0.7103956937789917\n",
            "Epoch 7, Loss: 0.6223320960998535\n",
            "Epoch 8, Loss: 0.7258370518684387\n",
            "Epoch 9, Loss: 0.6210140585899353\n",
            "Epoch 10, Loss: 0.7100357413291931\n",
            "Attacker Baseline Model Accuracy: 66.67%\n",
            "Attacker Private Model Accuracy: 66.89%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(attacker_baseline.state_dict(),os.path.join(path, 'main_attacker2.pth'))\n",
        "torch.save(attacker_private.state_dict(),os.path.join(path, 'private_attacker2.pth'))"
      ],
      "metadata": {
        "id": "wSqo_s7F1t5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseatt = ImprovedAttackerModel()\n",
        "priatt = ImprovedAttackerModel()\n",
        "baseatt.load_state_dict(torch.load(os.path.join(path, 'main_attacker2.pth')))\n",
        "priatt.load_state_dict(torch.load(os.path.join(path, 'private_attacker2.pth')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ir8awjjE9v_e",
        "outputId": "99ce0ac5-5c78-4a1b-9718-5f98fae388e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    }
  ]
}